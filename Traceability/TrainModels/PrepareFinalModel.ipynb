{
 "cells": [
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "So it seems that class 'None' obtains a high f1score and accuraccy.\n",
    "Therefore, what we will do is first train a classifier for every class considering the best results obtained. Then, apply all classifiers. If its tagged as belonging to None, then the sentence will be considered as not having anything to do with privacy polcies, then it will be passed through the other classifiers. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: 'Amazon Pay', 1: 'Device Address', 2: 'Device country and postal code', 3: 'Email Address', 4: 'Location Services', 5: 'Mobile Number', 6: 'Name', 7: 'Personal Information', 8: 'Skill Personisation', 9: 'None'}\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "indexToLabel = {0: 'Amazon Pay', 1: 'Device Address', 2: 'Device country and postal code', \n",
    "                3: 'Email Address', 4: 'Location Services', 5: 'Mobile Number', 6: 'Name', \n",
    "                7: 'Personal Information', 8: 'Skill Personisation', 9:'None'}\n",
    "labelToindex = {v:k for (k,v) in indexToLabel.items()}\n",
    "\n",
    "print(indexToLabel)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepare dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.metrics import confusion_matrix, f1_score\n",
    "import pickle\n",
    "\n",
    "'''\n",
    "prepare dataset\n",
    "1. Jide Alexa's excel file\n",
    "'''\n",
    "path = 'data/Annotated_Policies_Alexa.csv'\n",
    "df = pd.read_csv(path)\n",
    "df = df[pd.notnull(df['Permission'])]\n",
    "df = df[pd.notnull(df['Sentence'])]\n",
    "df.drop('Similarities_Score',axis=1,inplace=True)\n",
    "\n",
    "\n",
    "for c in df['Permission'].unique():\n",
    "    df.loc[ (df['Permission'] == c), 'Label'] = labelToindex[c]\n",
    "df['Label'] = np.int64(df['Label'])\n",
    "\n",
    "for index in indexToLabel.keys():\n",
    "    df.loc[df['Label'] == index, 'Label'+str(index)] = 0\n",
    "    df.loc[df['Label'] != index, 'Label'+str(index)] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Permission</th>\n",
       "      <th>Sentence</th>\n",
       "      <th>Label</th>\n",
       "      <th>Label0</th>\n",
       "      <th>Label1</th>\n",
       "      <th>Label2</th>\n",
       "      <th>Label3</th>\n",
       "      <th>Label4</th>\n",
       "      <th>Label5</th>\n",
       "      <th>Label6</th>\n",
       "      <th>Label7</th>\n",
       "      <th>Label8</th>\n",
       "      <th>Label9</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Amazon Pay</td>\n",
       "      <td>when you make a donation, amazon pay will proc...</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Amazon Pay</td>\n",
       "      <td>however, the amazon pay a-to-z guarantee cover...</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Amazon Pay</td>\n",
       "      <td>some other alexa skills allow you to purchase ...</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Amazon Pay</td>\n",
       "      <td>we will ask for standard credit card informati...</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Amazon Pay</td>\n",
       "      <td>transaction information : payment information ...</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Permission                                           Sentence  Label  \\\n",
       "0  Amazon Pay  when you make a donation, amazon pay will proc...      0   \n",
       "1  Amazon Pay  however, the amazon pay a-to-z guarantee cover...      0   \n",
       "2  Amazon Pay  some other alexa skills allow you to purchase ...      0   \n",
       "3  Amazon Pay  we will ask for standard credit card informati...      0   \n",
       "4  Amazon Pay  transaction information : payment information ...      0   \n",
       "\n",
       "   Label0  Label1  Label2  Label3  Label4  Label5  Label6  Label7  Label8  \\\n",
       "0     0.0     1.0     1.0     1.0     1.0     1.0     1.0     1.0     1.0   \n",
       "1     0.0     1.0     1.0     1.0     1.0     1.0     1.0     1.0     1.0   \n",
       "2     0.0     1.0     1.0     1.0     1.0     1.0     1.0     1.0     1.0   \n",
       "3     0.0     1.0     1.0     1.0     1.0     1.0     1.0     1.0     1.0   \n",
       "4     0.0     1.0     1.0     1.0     1.0     1.0     1.0     1.0     1.0   \n",
       "\n",
       "   Label9  \n",
       "0     1.0  \n",
       "1     1.0  \n",
       "2     1.0  \n",
       "3     1.0  \n",
       "4     1.0  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Permission</th>\n",
       "      <th>Sentence</th>\n",
       "      <th>Label</th>\n",
       "      <th>Label0</th>\n",
       "      <th>Label1</th>\n",
       "      <th>Label2</th>\n",
       "      <th>Label3</th>\n",
       "      <th>Label4</th>\n",
       "      <th>Label5</th>\n",
       "      <th>Label6</th>\n",
       "      <th>Label7</th>\n",
       "      <th>Label8</th>\n",
       "      <th>Label9</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>467</th>\n",
       "      <td>Device country and postal code</td>\n",
       "      <td>zip code</td>\n",
       "      <td>2</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                         Permission  Sentence  Label  Label0  Label1  Label2  \\\n",
       "467  Device country and postal code  zip code      2     1.0     1.0     0.0   \n",
       "\n",
       "     Label3  Label4  Label5  Label6  Label7  Label8  Label9  \n",
       "467     1.0     1.0     1.0     1.0     1.0     1.0     1.0  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.loc[df['Sentence'] == 'zip code']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classifier for class 'Amazon Pay' - 0.95+"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "columnToUse = 'Label0'\n",
    "\n",
    "classproportion = 2\n",
    "ngrams = (1,3)\n",
    "apply_tfidf = False\n",
    "apply_binary = True\n",
    "svm_alpha = 0.001\n",
    "svm_loss = 'modified_huber'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ORIGINAL DATASET SAMPLING\n",
      "\t 692 sentences of class 0;  10371 sentence for class 1\n",
      "REMOVING REPEATED SENTENCES FROM CLASS 1 THAT ALSO BELONG TO CLASS 0\n",
      "\tRemoved 90 repeated sentences from X1\n",
      "CHECKING THE PROPORTION OF INSTANCES IN EACH CLASS\n",
      "\tclass 1 is larger than 0, applying subsampling\n",
      "\t 692 sentences of class 0;  1384 sentence for class 1\n"
     ]
    }
   ],
   "source": [
    "X0 = df.loc[df[columnToUse] == 0]['Sentence'].tolist()\n",
    "X0 = X0*4\n",
    "xn0 = len(X0)\n",
    "X1 = df.loc[df[columnToUse] == 1]['Sentence'].tolist()\n",
    "xn1 = len(X1)\n",
    "\n",
    "print('ORIGINAL DATASET SAMPLING')\n",
    "print('\\t',xn0, 'sentences of class 0; ', xn1, 'sentence for class 1')\n",
    "\n",
    "print('REMOVING REPEATED SENTENCES FROM CLASS 1 THAT ALSO BELONG TO CLASS 0')\n",
    "#then check for repeated sentences belonging to both X0 and X1. If it belongs to X0, we remove it from X1.\n",
    "#This happens, sentences are repeated in the dataset as belonging to more than one privacy category, hence we clean it\n",
    "#before letting the classifier learn\n",
    "totalr = 0\n",
    "for s0 in X0:\n",
    "    s0 = s0.strip()\n",
    "    ir = [index for index, s in enumerate(X1) if s.strip()==s0]\n",
    "    ir = sorted(ir, reverse = True) #need to sort in reverse so when deleting an index does not affect the rest of indices\n",
    "    for index in ir:\n",
    "        del X1[index]\n",
    "    totalr += len(ir)\n",
    "print('\\tRemoved', totalr, 'repeated sentences from X1')\n",
    "\n",
    "#subsampling (random) of largest class\n",
    "print('CHECKING THE PROPORTION OF INSTANCES IN EACH CLASS')\n",
    "if(len(X1)>len(X0)*classproportion):\n",
    "    X1 = random.sample(X1, xn0*classproportion)\n",
    "    xn1 = len(X1)\n",
    "    print('\\tclass 1 is larger than 0, applying subsampling')\n",
    "elif(len(X0)>len(X1)*classproportion):\n",
    "    X0 = random.sample(X0, xn1*classproportion)\n",
    "    xn0 = len(X0)\n",
    "    print('\\tclass 0 is larger than 1')\n",
    "print('\\t',xn0, 'sentences of class 0; ', xn1, 'sentence for class 1')\n",
    "\n",
    "X = X0+X1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">Train: 0=484, 1=969, Test: 0=208, 1=415\n",
      "CM [[208   0]\n",
      " [  0 415]]\n",
      "F1 1.0\n",
      "Acc 1.0\n"
     ]
    }
   ],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "text_clf = Pipeline([\n",
    "    ('vect', CountVectorizer(ngram_range=ngrams, binary=apply_binary)),\n",
    "    ('tfidf', TfidfTransformer(use_idf=apply_tfidf)),\n",
    "    ('clf', SGDClassifier(loss=svm_loss, penalty='l2',\n",
    "                          alpha=svm_alpha, random_state=42,\n",
    "                          max_iter=10000, tol=None,  class_weight='balanced')),\n",
    "    ])\n",
    "\n",
    "labels = xn0*[0] + xn1*[1]\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, labels, test_size=0.30, stratify=labels)\n",
    "\n",
    "#print proportion for training and testing\n",
    "X_train=np.array(X_train);X_test=np.array(X_test);y_train=np.array(y_train);y_test=np.array(y_test);\n",
    "train_0, train_1 = len(y_train[y_train==0]), len(y_train[y_train==1])\n",
    "test_0, test_1 = len(y_test[y_test==0]), len(y_test[y_test==1])\n",
    "print('>Train: 0=%d, 1=%d, Test: 0=%d, 1=%d' % (train_0, train_1, test_0, test_1))\n",
    "\n",
    "\n",
    "#train and test\n",
    "text_clf.fit(X_train, y_train)          \n",
    "pred_y = text_clf.predict(X_test)\n",
    "#plot_confusion_matrix(clf, test_X, test_y)\n",
    "cm = confusion_matrix(y_test, pred_y)\n",
    "f1 = f1_score(y_test, pred_y, average=None)[0]\n",
    "acc = np.mean(pred_y == y_test) \n",
    "\n",
    "print('CM', cm)\n",
    "print('F1', f1)\n",
    "print('Acc', acc)\n",
    "\n",
    "#save model\n",
    "pkl_filename = 'models/'+columnToUse+\"_new_model.pkl\"\n",
    "with open(pkl_filename, 'wb') as file:\n",
    "    pickle.dump(text_clf, file)\n",
    "    print('*Model for ', columnToUse, 'saved in ', pkl_filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test without ignoring 'None' sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ORIGINAL DATASET SAMPLING\n",
      "\t 10544 instances to classify,  173 for class 0,  10371 for class 1\n",
      "REMOVING REPEATED SENTENCES FROM CLASS 1 THAT ALSO BELONG TO CLASS 0\n",
      "\tRemoved 90 repeated sentences from X1\n",
      "REMOVING SENTENCES USED TO TRAIN THE MODEL THAT BELONG TO CLASS 0\n",
      "\tRemoved 1385 sentences used to train the model; sentences from test set X\n",
      "CM [[  21    0]\n",
      " [  65 8983]]\n",
      "F1 0.3925233644859813\n",
      "Acc 0.9928327268717609\n"
     ]
    }
   ],
   "source": [
    "#Load classifier for 'amazon pay' and test it on the entire dataset\n",
    "pkl_filename = 'models/'+columnToUse+\"_new_model.pkl\"\n",
    "with open(pkl_filename, 'rb') as file:\n",
    "    text_clf = pickle.load(file)\n",
    "    \n",
    "#get the data\n",
    "#ignoring None sentences\n",
    "#X = df.loc[df['Permission'] != 'None', 'Sentence']\n",
    "#y = df.loc[df['Permission'] != 'None', columnToUse]\n",
    "X0 = df.loc[df[columnToUse] == 0]['Sentence'].tolist()\n",
    "xn0 = len(X0)\n",
    "X1 = df.loc[df[columnToUse] == 1]['Sentence'].tolist()\n",
    "xn1 = len(X1)\n",
    "print('ORIGINAL DATASET SAMPLING')\n",
    "print('\\t',len(X0+X1), 'instances to classify, ',xn0, 'for class 0, ', xn1, 'for class 1' )\n",
    "print('REMOVING REPEATED SENTENCES FROM CLASS 1 THAT ALSO BELONG TO CLASS 0')\n",
    "\n",
    "#then check for repeated sentences belonging to both X0 and X1. If it belongs to X0, we remove it from X1.\n",
    "#This happens, sentences are repeated in the dataset as belonging to more than one privacy category, hence we clean it\n",
    "#before letting the classifier learn\n",
    "totalr = 0\n",
    "for s0 in X0:\n",
    "    s0 = s0.strip()\n",
    "    ir = [index for index, s in enumerate(X1) if s.strip()==s0]\n",
    "    ir = sorted(ir, reverse = True) #need to sort in reverse so when deleting an index does not affect the rest of indices\n",
    "    for index in ir:\n",
    "        del X1[index]\n",
    "    totalr += len(ir)\n",
    "print('\\tRemoved', totalr, 'repeated sentences from X1')\n",
    "X = X0+X1\n",
    "xn0 = len(X0);xn1 = len(X1)\n",
    "y = xn0*[0] + xn1*[1]\n",
    "\n",
    "#test the data\n",
    "pred_y = text_clf.predict(X)\n",
    "cm = confusion_matrix(y, pred_y)\n",
    "f1 = f1_score(y, pred_y, average=None)[0]\n",
    "acc = np.mean(pred_y == y)\n",
    "\n",
    "print('CM', cm)\n",
    "print('F1', f1)\n",
    "print('Acc', acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test ignoring None sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ORIGINAL DATASET SAMPLING\n",
      "\t 4079 instances to classify,  173 for class 0,  3906 for class 1\n",
      "REMOVING REPEATED SENTENCES FROM CLASS 1 THAT ALSO BELONG TO CLASS 0\n",
      "\tRemoved 90 repeated sentences from X1\n",
      "REMOVING SENTENCES USED TO TRAIN THE MODEL THAT BELONG TO CLASS 0\n",
      "\tRemoved 652 sentences used to train the model; sentences from test set X\n",
      "CM [[  21    0]\n",
      " [  49 3267]]\n",
      "F1 0.4615384615384615\n",
      "Acc 0.9853161522325442\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#Load classifier for 'amazon pay' and test it on the entire dataset\n",
    "pkl_filename = 'models/'+columnToUse+\"_new_model.pkl\"\n",
    "with open(pkl_filename, 'rb') as file:\n",
    "    text_clf = pickle.load(file)\n",
    "    \n",
    "#get the data\n",
    "#ignoring None sentences\n",
    "#X = df.loc[df['Permission'] != 'None', 'Sentence']\n",
    "#y = df.loc[df['Permission'] != 'None', columnToUse]\n",
    "X0 = df.loc[(df[columnToUse] == 0)&(df['Permission'] != 'None')]['Sentence'].tolist()\n",
    "xn0 = len(X0)\n",
    "X1 = df.loc[(df[columnToUse] == 1)&(df['Permission'] != 'None')]['Sentence'].tolist()\n",
    "xn1 = len(X1)\n",
    "print('ORIGINAL DATASET SAMPLING')\n",
    "print('\\t',len(X0+X1), 'instances to classify, ',xn0, 'for class 0, ', xn1, 'for class 1' )\n",
    "print('REMOVING REPEATED SENTENCES FROM CLASS 1 THAT ALSO BELONG TO CLASS 0')\n",
    "#then check for repeated sentences belonging to both X0 and X1. If it belongs to X0, we remove it from X1.\n",
    "#This happens, sentences are repeated in the dataset as belonging to more than one privacy category, hence we clean it\n",
    "#before letting the classifier learn\n",
    "totalr = 0\n",
    "for s0 in X0:\n",
    "    s0 = s0.strip()\n",
    "    ir = [index for index, s in enumerate(X1) if s.strip()==s0]\n",
    "    ir = sorted(ir, reverse = True) #need to sort in reverse so when deleting an index does not affect the rest of indices\n",
    "    for index in ir:\n",
    "        del X1[index]\n",
    "    totalr += len(ir)\n",
    "print('\\tRemoved', totalr, 'repeated sentences from X1')\n",
    "X = X0+X1\n",
    "xn0 = len(X0);xn1 = len(X1)\n",
    "y = xn0*[0] + xn1*[1]\n",
    "\n",
    "\n",
    "#test the data\n",
    "pred_y = text_clf.predict(X)\n",
    "cm = confusion_matrix(y, pred_y)\n",
    "f1 = f1_score(y, pred_y, average=None)[0]\n",
    "acc = np.mean(pred_y == y)\n",
    "\n",
    "print('CM', cm)\n",
    "print('F1', f1)\n",
    "print('Acc', acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Classifier for class 'Device Address' - 0.80"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device Address  -  1  -  Label1\n"
     ]
    }
   ],
   "source": [
    "categoryname = 'Device Address'\n",
    "labelindex = labelToindex[categoryname]\n",
    "columnToUse = 'Label'+str(labelindex)\n",
    "print(categoryname, ' - ', labelindex, ' - ', columnToUse)\n",
    "\n",
    "classproportion = 3\n",
    "ngrams = (1,3)\n",
    "apply_tfidf = False\n",
    "apply_binary = True\n",
    "svm_loss = 'modified_huber'\n",
    "svm_alpha = 0.001\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ORIGINAL DATASET SAMPLING\n",
      "\t 279 sentences of class 0;  10265 sentence for class 1\n",
      "INCREASING DATASET WITH APP350\n",
      "\t 79 new sentences found in  data/App350/Contact_Postal_Address_3rdParty.txt\n",
      "\t 431 new sentences found in  data/App350/Contact_Postal_Address_1stParty.txt\n",
      "\t 1578 sentences of class 0;  10265 sentence for class 1\n",
      "REMOVING REPEATED SENTENCES FROM CLASS 1 THAT ALSO BELONG TO CLASS 0\n",
      "\tRemoved 256 repeated sentences from X1\n",
      "CHECKING THE PROPORTION OF INSTANCES IN EACH CLASS\n",
      "\tclass 1 is larger than 0, applying subsampling\n",
      "\t 1578 sentences of class 0;  4734 sentence for class 1\n"
     ]
    }
   ],
   "source": [
    "dfaux = df\n",
    "\n",
    "X0 = dfaux.loc[dfaux[columnToUse] == 0]['Sentence'].tolist()\n",
    "xn0 = len(X0)\n",
    "X1 = dfaux.loc[dfaux[columnToUse] == 1]['Sentence'].tolist()\n",
    "xn1 = len(X1)\n",
    "\n",
    "print('ORIGINAL DATASET SAMPLING')\n",
    "print('\\t',xn0, 'sentences of class 0; ', xn1, 'sentence for class 1')\n",
    "\n",
    "print('INCREASING DATASET WITH APP350')\n",
    "relevantfiles = ['data/App350/Contact_Postal_Address_3rdParty.txt', 'data/App350/Contact_Postal_Address_1stParty.txt']\n",
    "for filepath in relevantfiles:\n",
    "    with open(filepath, encoding='utf8') as f:   #utf8      \n",
    "        text = f.readlines()\n",
    "        text = [k.replace('\\n', '').strip() for k in text]\n",
    "        print('\\t', len(text), 'new sentences found in ', filepath)\n",
    "        for t in text:\n",
    "            dfaux = dfaux.append({'Permission':categoryname, 'Sentence':t, 'Label':labelindex, columnToUse:0},  ignore_index=True)\n",
    "            \n",
    "X0 = dfaux.loc[dfaux[columnToUse] == 0]['Sentence'].tolist()\n",
    "X0 = X0*2\n",
    "xn0 = len(X0)\n",
    "X1 = dfaux.loc[dfaux[columnToUse] == 1]['Sentence'].tolist()\n",
    "xn1 = len(X1)\n",
    "print('\\t',xn0, 'sentences of class 0; ', xn1, 'sentence for class 1')\n",
    "            \n",
    "            \n",
    "print('REMOVING REPEATED SENTENCES FROM CLASS 1 THAT ALSO BELONG TO CLASS 0')\n",
    "#then check for repeated sentences belonging to both X0 and X1. If it belongs to X0, we remove it from X1.\n",
    "#This happens, sentences are repeated in the dataset as belonging to more than one privacy category, hence we clean it\n",
    "#before letting the classifier learn\n",
    "totalr = 0\n",
    "for s0 in X0:\n",
    "    s0 = s0.strip()\n",
    "    ir = [index for index, s in enumerate(X1) if s.strip()==s0]\n",
    "    ir = sorted(ir, reverse = True) #need to sort in reverse so when deleting an index does not affect the rest of indices\n",
    "    for index in ir:\n",
    "        del X1[index]\n",
    "    totalr += len(ir)\n",
    "print('\\tRemoved', totalr, 'repeated sentences from X1')\n",
    "\n",
    "#subsampling (random) of largest class\n",
    "print('CHECKING THE PROPORTION OF INSTANCES IN EACH CLASS')\n",
    "if(len(X1)>len(X0)*classproportion):\n",
    "    X1 = random.sample(X1, xn0*classproportion)\n",
    "    xn1 = len(X1)\n",
    "    print('\\tclass 1 is larger than 0, applying subsampling')\n",
    "elif(len(X0)>len(X1)*classproportion):\n",
    "    X0 = random.sample(X0, xn1*classproportion)\n",
    "    xn0 = len(X0)\n",
    "    print('\\tclass 0 is larger than 1')\n",
    "print('\\t',xn0, 'sentences of class 0; ', xn1, 'sentence for class 1')\n",
    "X = X0+X1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">Train: 0=1499, 1=4497, Test: 0=79, 1=237\n",
      "CM [[ 75   4]\n",
      " [ 12 225]]\n",
      "F1 0.9036144578313253\n",
      "Acc 0.9493670886075949\n",
      "*Model for  Label1 saved in  models/Label1_new_model.pkl\n"
     ]
    }
   ],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "text_clf = Pipeline([\n",
    "    ('vect', CountVectorizer(ngram_range=ngrams, binary=apply_binary)),\n",
    "    ('tfidf', TfidfTransformer(use_idf=apply_tfidf)),\n",
    "    ('clf', SGDClassifier(loss=svm_loss, penalty='l2',\n",
    "                          alpha=svm_alpha, max_iter=10000, tol=None,  class_weight='balanced')),\n",
    "    ])\n",
    "\n",
    "labels = xn0*[0] + xn1*[1]\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, labels, test_size=0.05, stratify=labels)\n",
    "\n",
    "#print proportion for training and testing\n",
    "X_train=np.array(X_train);X_test=np.array(X_test);y_train=np.array(y_train);y_test=np.array(y_test);\n",
    "train_0, train_1 = len(y_train[y_train==0]), len(y_train[y_train==1])\n",
    "test_0, test_1 = len(y_test[y_test==0]), len(y_test[y_test==1])\n",
    "print('>Train: 0=%d, 1=%d, Test: 0=%d, 1=%d' % (train_0, train_1, test_0, test_1))\n",
    "\n",
    "\n",
    "#train and test\n",
    "text_clf.fit(X_train, y_train)          \n",
    "pred_y = text_clf.predict(X_test)\n",
    "#plot_confusion_matrix(clf, test_X, test_y)\n",
    "cm = confusion_matrix(y_test, pred_y)\n",
    "f1 = f1_score(y_test, pred_y, average=None)[0]\n",
    "acc = np.mean(pred_y == y_test) \n",
    "\n",
    "print('CM', cm)\n",
    "print('F1', f1)\n",
    "print('Acc', acc)\n",
    "\n",
    "#save model\n",
    "pkl_filename = 'models/'+columnToUse+\"_new_model.pkl\"\n",
    "with open(pkl_filename, 'wb') as file:\n",
    "    pickle.dump(text_clf, file)\n",
    "    print('*Model for ', columnToUse, 'saved in ', pkl_filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "### Test without ignoring 'None' sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ORIGINAL DATASET SAMPLING\n",
      "\t 11054 instances to classify,  789 for class 0,  10265 for class 1\n",
      "REMOVING REPEATED SENTENCES FROM CLASS 1 THAT ALSO BELONG TO CLASS 0\n",
      "\tRemoved 256 repeated sentences from X1\n",
      "CM [[ 784    5]\n",
      " [ 534 9475]]\n",
      "F1 0.7441860465116279\n",
      "Acc 0.9500833487682904\n"
     ]
    }
   ],
   "source": [
    "#Load classifier for 'amazon pay' and test it on the entire dataset\n",
    "pkl_filename = 'models/'+columnToUse+\"_new_model.pkl\"\n",
    "with open(pkl_filename, 'rb') as file:\n",
    "    text_clf = pickle.load(file)\n",
    "    \n",
    "#get the data\n",
    "#ignoring None sentences\n",
    "#X = dfaux.loc[dfaux['Permission'] != 'None', 'Sentence']\n",
    "#y = dfaux.loc[dfaux['Permission'] != 'None', columnToUse]\n",
    "X0 = dfaux.loc[dfaux[columnToUse] == 0]['Sentence'].tolist()\n",
    "xn0 = len(X0)\n",
    "X1 = dfaux.loc[dfaux[columnToUse] == 1]['Sentence'].tolist()\n",
    "xn1 = len(X1)\n",
    "print('ORIGINAL DATASET SAMPLING')\n",
    "print('\\t',len(X0+X1), 'instances to classify, ',xn0, 'for class 0, ', xn1, 'for class 1' )\n",
    "print('REMOVING REPEATED SENTENCES FROM CLASS 1 THAT ALSO BELONG TO CLASS 0')\n",
    "#then check for repeated sentences belonging to both X0 and X1. If it belongs to X0, we remove it from X1.\n",
    "#This happens, sentences are repeated in the dataset as belonging to more than one privacy category, hence we clean it\n",
    "#before letting the classifier learn\n",
    "totalr = 0\n",
    "for s0 in X0:\n",
    "    s0 = s0.strip()\n",
    "    ir = [index for index, s in enumerate(X1) if s.strip()==s0]\n",
    "    ir = sorted(ir, reverse = True) #need to sort in reverse so when deleting an index does not affect the rest of indices\n",
    "    for index in ir:\n",
    "        del X1[index]\n",
    "    totalr += len(ir)\n",
    "print('\\tRemoved', totalr, 'repeated sentences from X1')\n",
    "X = X0+X1\n",
    "xn0 = len(X0);xn1 = len(X1)\n",
    "y = xn0*[0] + xn1*[1]\n",
    "\n",
    "#test the data\n",
    "pred_y = text_clf.predict(X)\n",
    "cm = confusion_matrix(y, pred_y)\n",
    "f1 = f1_score(y, pred_y, average=None)[0]\n",
    "acc = np.mean(pred_y == y)\n",
    "\n",
    "print('CM', cm)\n",
    "print('F1', f1)\n",
    "print('Acc', acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "### Test ignoring None sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ORIGINAL DATASET SAMPLING\n",
      "\t 4589 instances to classify,  789 for class 0,  3800 for class 1\n",
      "REMOVING REPEATED SENTENCES FROM CLASS 1 THAT ALSO BELONG TO CLASS 0\n",
      "\tRemoved 252 repeated sentences from X1\n",
      "CM [[ 784    5]\n",
      " [ 503 3045]]\n",
      "F1 0.7552986512524086\n",
      "Acc 0.8828683421720083\n"
     ]
    }
   ],
   "source": [
    "#Load classifier for 'amazon pay' and test it on the entire dataset\n",
    "pkl_filename = 'models/'+columnToUse+\"_new_model.pkl\"\n",
    "with open(pkl_filename, 'rb') as file:\n",
    "    text_clf = pickle.load(file)\n",
    "    \n",
    "#get the data\n",
    "#ignoring None sentences\n",
    "#X = dfaux.loc[dfaux['Permission'] != 'None', 'Sentence']\n",
    "#y = dfaux.loc[dfaux['Permission'] != 'None', columnToUse]\n",
    "X0 = dfaux.loc[(dfaux[columnToUse] == 0)&(dfaux['Permission'] != 'None')]['Sentence'].tolist()\n",
    "xn0 = len(X0)\n",
    "X1 = dfaux.loc[(dfaux[columnToUse] == 1)&(dfaux['Permission'] != 'None')]['Sentence'].tolist()\n",
    "xn1 = len(X1)\n",
    "print('ORIGINAL DATASET SAMPLING')\n",
    "print('\\t',len(X0+X1), 'instances to classify, ',xn0, 'for class 0, ', xn1, 'for class 1' )\n",
    "print('REMOVING REPEATED SENTENCES FROM CLASS 1 THAT ALSO BELONG TO CLASS 0')\n",
    "#then check for repeated sentences belonging to both X0 and X1. If it belongs to X0, we remove it from X1.\n",
    "#This happens, sentences are repeated in the dataset as belonging to more than one privacy category, hence we clean it\n",
    "#before letting the classifier learn\n",
    "totalr = 0\n",
    "for s0 in X0:\n",
    "    s0 = s0.strip()\n",
    "    ir = [index for index, s in enumerate(X1) if s.strip()==s0]\n",
    "    ir = sorted(ir, reverse = True) #need to sort in reverse so when deleting an index does not affect the rest of indices\n",
    "    for index in ir:\n",
    "        del X1[index]\n",
    "    totalr += len(ir)\n",
    "print('\\tRemoved', totalr, 'repeated sentences from X1')\n",
    "X = X0+X1\n",
    "xn0 = len(X0);xn1 = len(X1)\n",
    "y = xn0*[0] + xn1*[1]\n",
    "\n",
    "#test the data\n",
    "pred_y = text_clf.predict(X)\n",
    "cm = confusion_matrix(y, pred_y)\n",
    "f1 = f1_score(y, pred_y, average=None)[0]\n",
    "acc = np.mean(pred_y == y)\n",
    "\n",
    "print('CM', cm)\n",
    "print('F1', f1)\n",
    "print('Acc', acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Classifier for class 'Device country and postal code' - 0.88"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device country and postal code  -  2  -  Label2\n"
     ]
    }
   ],
   "source": [
    "categoryname = 'Device country and postal code'\n",
    "labelindex = labelToindex[categoryname]\n",
    "columnToUse = 'Label'+str(labelindex)\n",
    "print(categoryname, ' - ', labelindex, ' - ', columnToUse)\n",
    "\n",
    "classproportion = 2\n",
    "ngrams = (1,3)\n",
    "apply_tfidf = False\n",
    "apply_binary = True\n",
    "svm_loss = 'modified_huber'\n",
    "svm_alpha = 0.0001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ORIGINAL DATASET SAMPLING\n",
      "\t 251 sentences of class 0;  10293 sentence for class 1\n",
      "INCREASING DATASET WITH APP350\n",
      "\t 93 new sentences found in  data/App350/Contact_City_1stParty.txt\n",
      "\t 19 new sentences found in  data/App350/Contact_City_3rdParty.txt\n",
      "\t 140 new sentences found in  data/App350/Contact_ZIP_1stParty.txt\n",
      "\t 22 new sentences found in  data/App350/Contact_ZIP_3rdParty.txt\n",
      "\t 2100 sentences of class 0;  10293 sentence for class 1\n",
      "REMOVING REPEATED SENTENCES FROM CLASS 1 THAT ALSO BELONG TO CLASS 0\n",
      "\tRemoved 92 repeated sentences from X1\n",
      "CHECKING THE PROPORTION OF INSTANCES IN EACH CLASS\n",
      "\tclass 1 is larger than 0, applying subsampling\n",
      "\t 2100 sentences of class 0;  4200 sentence for class 1\n"
     ]
    }
   ],
   "source": [
    "dfaux = df\n",
    "\n",
    "X0 = dfaux.loc[dfaux[columnToUse] == 0]['Sentence'].tolist()\n",
    "xn0 = len(X0)\n",
    "X1 = dfaux.loc[dfaux[columnToUse] == 1]['Sentence'].tolist()\n",
    "xn1 = len(X1)\n",
    "\n",
    "print('ORIGINAL DATASET SAMPLING')\n",
    "print('\\t',xn0, 'sentences of class 0; ', xn1, 'sentence for class 1')\n",
    "\n",
    "print('INCREASING DATASET WITH APP350')\n",
    "relevantfiles = ['data/App350/Contact_City_1stParty.txt',\n",
    "                 'data/App350/Contact_City_3rdParty.txt',\n",
    "                 'data/App350/Contact_ZIP_1stParty.txt',\n",
    "                 'data/App350/Contact_ZIP_3rdParty.txt']\n",
    "for filepath in relevantfiles:\n",
    "    with open(filepath, encoding='utf8') as f:   #utf8      \n",
    "        text = f.readlines()\n",
    "        text = [k.replace('\\n', '').strip() for k in text]\n",
    "        print('\\t', len(text), 'new sentences found in ', filepath)\n",
    "        for t in text:\n",
    "            dfaux = dfaux.append({'Permission':categoryname, 'Sentence':t, 'Label':labelindex, columnToUse:0},  ignore_index=True)\n",
    "            \n",
    "X0 = dfaux.loc[dfaux[columnToUse] == 0]['Sentence'].tolist()\n",
    "X0 = X0*4\n",
    "xn0 = len(X0)\n",
    "X1 = dfaux.loc[dfaux[columnToUse] == 1]['Sentence'].tolist()\n",
    "xn1 = len(X1)\n",
    "print('\\t',xn0, 'sentences of class 0; ', xn1, 'sentence for class 1')\n",
    "            \n",
    "            \n",
    "print('REMOVING REPEATED SENTENCES FROM CLASS 1 THAT ALSO BELONG TO CLASS 0')\n",
    "#then check for repeated sentences belonging to both X0 and X1. If it belongs to X0, we remove it from X1.\n",
    "#This happens, sentences are repeated in the dataset as belonging to more than one privacy category, hence we clean it\n",
    "#before letting the classifier learn\n",
    "totalr = 0\n",
    "for s0 in X0:\n",
    "    s0 = s0.strip()\n",
    "    ir = [index for index, s in enumerate(X1) if s.strip()==s0]\n",
    "    ir = sorted(ir, reverse = True) #need to sort in reverse so when deleting an index does not affect the rest of indices\n",
    "    for index in ir:\n",
    "        del X1[index]\n",
    "    totalr += len(ir)\n",
    "print('\\tRemoved', totalr, 'repeated sentences from X1')\n",
    "\n",
    "#subsampling (random) of largest class\n",
    "print('CHECKING THE PROPORTION OF INSTANCES IN EACH CLASS')\n",
    "if(len(X1)>len(X0)*classproportion):\n",
    "    X1 = random.sample(X1, xn0*classproportion)\n",
    "    xn1 = len(X1)\n",
    "    print('\\tclass 1 is larger than 0, applying subsampling')\n",
    "elif(len(X0)>len(X1)*classproportion):\n",
    "    X0 = random.sample(X0, xn1*classproportion)\n",
    "    xn0 = len(X0)\n",
    "    print('\\tclass 0 is larger than 1')\n",
    "print('\\t',xn0, 'sentences of class 0; ', xn1, 'sentence for class 1')\n",
    "X = X0+X1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">Train: 0=965, 1=1930, Test: 0=51, 1=102\n",
      "CM [[51  0]\n",
      " [ 4 98]]\n",
      "F1 0.9622641509433962\n",
      "Acc 0.9738562091503268\n",
      "*Model for  Label2 saved in  models/Label2_new_model.pkl\n"
     ]
    }
   ],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "text_clf = Pipeline([\n",
    "    ('vect', CountVectorizer(ngram_range=ngrams, binary=apply_binary)),\n",
    "    ('tfidf', TfidfTransformer(use_idf=apply_tfidf)),\n",
    "    ('clf', SGDClassifier(loss=svm_loss, penalty='l2',\n",
    "                          alpha=svm_alpha, max_iter=10000, tol=None,  class_weight='balanced')),\n",
    "    ])\n",
    "\n",
    "labels = xn0*[0] + xn1*[1]\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, labels, test_size=0.05, stratify=labels)\n",
    "\n",
    "#print proportion for training and testing\n",
    "X_train=np.array(X_train);X_test=np.array(X_test);y_train=np.array(y_train);y_test=np.array(y_test);\n",
    "train_0, train_1 = len(y_train[y_train==0]), len(y_train[y_train==1])\n",
    "test_0, test_1 = len(y_test[y_test==0]), len(y_test[y_test==1])\n",
    "print('>Train: 0=%d, 1=%d, Test: 0=%d, 1=%d' % (train_0, train_1, test_0, test_1))\n",
    "\n",
    "\n",
    "#train and test\n",
    "text_clf.fit(X_train, y_train)          \n",
    "pred_y = text_clf.predict(X_test)\n",
    "#plot_confusion_matrix(clf, test_X, test_y)\n",
    "cm = confusion_matrix(y_test, pred_y)\n",
    "f1 = f1_score(y_test, pred_y, average=None)[0]\n",
    "acc = np.mean(pred_y == y_test) \n",
    "\n",
    "print('CM', cm)\n",
    "print('F1', f1)\n",
    "print('Acc', acc)\n",
    "\n",
    "#save model\n",
    "pkl_filename = 'models/'+columnToUse+\"_new_model.pkl\"\n",
    "with open(pkl_filename, 'wb') as file:\n",
    "    pickle.dump(text_clf, file)\n",
    "    print('*Model for ', columnToUse, 'saved in ', pkl_filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "### Test without ignoring 'None' sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ORIGINAL DATASET SAMPLING\n",
      "\t 10547 instances to classify,  254 for class 0,  10293 for class 1\n",
      "REMOVING REPEATED SENTENCES FROM CLASS 1 THAT ALSO BELONG TO CLASS 0\n",
      "\tRemoved 92 repeated sentences from X1\n",
      "CM [[  254     0]\n",
      " [  170 10031]]\n",
      "F1 0.7492625368731564\n",
      "Acc 0.983739837398374\n"
     ]
    }
   ],
   "source": [
    "#Load classifier for 'amazon pay' and test it on the entire dataset\n",
    "pkl_filename = 'models/'+columnToUse+\"_new_model.pkl\"\n",
    "with open(pkl_filename, 'rb') as file:\n",
    "    text_clf = pickle.load(file)\n",
    "    \n",
    "#get the data\n",
    "#ignoring None sentences\n",
    "#X = dfaux.loc[dfaux['Permission'] != 'None', 'Sentence']\n",
    "#y = dfaux.loc[dfaux['Permission'] != 'None', columnToUse]\n",
    "X0 = dfaux.loc[dfaux[columnToUse] == 0]['Sentence'].tolist()\n",
    "xn0 = len(X0)\n",
    "X1 = dfaux.loc[dfaux[columnToUse] == 1]['Sentence'].tolist()\n",
    "xn1 = len(X1)\n",
    "print('ORIGINAL DATASET SAMPLING')\n",
    "print('\\t',len(X0+X1), 'instances to classify, ',xn0, 'for class 0, ', xn1, 'for class 1' )\n",
    "print('REMOVING REPEATED SENTENCES FROM CLASS 1 THAT ALSO BELONG TO CLASS 0')\n",
    "#then check for repeated sentences belonging to both X0 and X1. If it belongs to X0, we remove it from X1.\n",
    "#This happens, sentences are repeated in the dataset as belonging to more than one privacy category, hence we clean it\n",
    "#before letting the classifier learn\n",
    "totalr = 0\n",
    "for s0 in X0:\n",
    "    s0 = s0.strip()\n",
    "    ir = [index for index, s in enumerate(X1) if s.strip()==s0]\n",
    "    ir = sorted(ir, reverse = True) #need to sort in reverse so when deleting an index does not affect the rest of indices\n",
    "    for index in ir:\n",
    "        del X1[index]\n",
    "    totalr += len(ir)\n",
    "print('\\tRemoved', totalr, 'repeated sentences from X1')\n",
    "X = X0+X1\n",
    "xn0 = len(X0);xn1 = len(X1)\n",
    "y = xn0*[0] + xn1*[1]\n",
    "\n",
    "#test the data\n",
    "pred_y = text_clf.predict(X)\n",
    "cm = confusion_matrix(y, pred_y)\n",
    "f1 = f1_score(y, pred_y, average=None)[0]\n",
    "acc = np.mean(pred_y == y)\n",
    "\n",
    "print('CM', cm)\n",
    "print('F1', f1)\n",
    "print('Acc', acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "### Test ignoring 'None' sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ORIGINAL DATASET SAMPLING\n",
      "\t 4082 instances to classify,  254 for class 0,  3828 for class 1\n",
      "REMOVING REPEATED SENTENCES FROM CLASS 1 THAT ALSO BELONG TO CLASS 0\n",
      "\tRemoved 91 repeated sentences from X1\n",
      "CM [[ 254    0]\n",
      " [ 145 3592]]\n",
      "F1 0.7779479326186829\n",
      "Acc 0.9636682535705337\n"
     ]
    }
   ],
   "source": [
    "#Load classifier for 'amazon pay' and test it on the entire dataset\n",
    "pkl_filename = 'models/'+columnToUse+\"_new_model.pkl\"\n",
    "with open(pkl_filename, 'rb') as file:\n",
    "    text_clf = pickle.load(file)\n",
    "    \n",
    "#get the data\n",
    "#ignoring None sentences\n",
    "#X = dfaux.loc[dfaux['Permission'] != 'None', 'Sentence']\n",
    "#y = dfaux.loc[dfaux['Permission'] != 'None', columnToUse]\n",
    "X0 = dfaux.loc[(dfaux[columnToUse] == 0)&(dfaux['Permission'] != 'None')]['Sentence'].tolist()\n",
    "xn0 = len(X0)\n",
    "X1 = dfaux.loc[(dfaux[columnToUse] == 1)&(dfaux['Permission'] != 'None')]['Sentence'].tolist()\n",
    "xn1 = len(X1)\n",
    "print('ORIGINAL DATASET SAMPLING')\n",
    "print('\\t',len(X0+X1), 'instances to classify, ',xn0, 'for class 0, ', xn1, 'for class 1' )\n",
    "print('REMOVING REPEATED SENTENCES FROM CLASS 1 THAT ALSO BELONG TO CLASS 0')\n",
    "#then check for repeated sentences belonging to both X0 and X1. If it belongs to X0, we remove it from X1.\n",
    "#This happens, sentences are repeated in the dataset as belonging to more than one privacy category, hence we clean it\n",
    "#before letting the classifier learn\n",
    "totalr = 0\n",
    "for s0 in X0:\n",
    "    s0 = s0.strip()\n",
    "    ir = [index for index, s in enumerate(X1) if s.strip()==s0]\n",
    "    ir = sorted(ir, reverse = True) #need to sort in reverse so when deleting an index does not affect the rest of indices\n",
    "    for index in ir:\n",
    "        del X1[index]\n",
    "    totalr += len(ir)\n",
    "print('\\tRemoved', totalr, 'repeated sentences from X1')\n",
    "X = X0+X1\n",
    "xn0 = len(X0);xn1 = len(X1)\n",
    "y = xn0*[0] + xn1*[1]\n",
    "\n",
    "#test the data\n",
    "pred_y = text_clf.predict(X)\n",
    "cm = confusion_matrix(y, pred_y)\n",
    "f1 = f1_score(y, pred_y, average=None)[0]\n",
    "acc = np.mean(pred_y == y)\n",
    "\n",
    "print('CM', cm)\n",
    "print('F1', f1)\n",
    "print('Acc', acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Classifier for class 'Email Address' - 0.9+"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Email Address  -  3  -  Label3\n"
     ]
    }
   ],
   "source": [
    "categoryname = 'Email Address'\n",
    "labelindex = labelToindex[categoryname]\n",
    "columnToUse = 'Label'+str(labelindex)\n",
    "print(categoryname, ' - ', labelindex, ' - ', columnToUse)\n",
    "\n",
    "classproportion = 2\n",
    "ngrams = (1,3)\n",
    "apply_tfidf = False\n",
    "apply_binary = True\n",
    "svm_loss = 'modified_huber'\n",
    "svm_alpha = 0.0001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ORIGINAL DATASET SAMPLING\n",
      "\t 232 sentences of class 0;  10312 sentence for class 1\n",
      "INCREASING DATASET WITH APP350\n",
      "\t 1804 new sentences found in  data/App350/Contact_E_Mail_Address_1stParty.txt\n",
      "\t 172 new sentences found in  data/App350/Contact_E_Mail_Address_3rdParty.txt\n",
      "\t 2208 sentences of class 0;  10312 sentence for class 1\n",
      "REMOVING REPEATED SENTENCES FROM CLASS 1 THAT ALSO BELONG TO CLASS 0\n",
      "\tRemoved 270 repeated sentences from X1\n",
      "CHECKING THE PROPORTION OF INSTANCES IN EACH CLASS\n",
      "\tclass 1 is larger than 0, applying subsampling\n",
      "\t 2208 sentences of class 0;  4416 sentence for class 1\n"
     ]
    }
   ],
   "source": [
    "dfaux = df\n",
    "\n",
    "X0 = dfaux.loc[dfaux[columnToUse] == 0]['Sentence'].tolist()\n",
    "xn0 = len(X0)\n",
    "X1 = dfaux.loc[dfaux[columnToUse] == 1]['Sentence'].tolist()\n",
    "xn1 = len(X1)\n",
    "\n",
    "print('ORIGINAL DATASET SAMPLING')\n",
    "print('\\t',xn0, 'sentences of class 0; ', xn1, 'sentence for class 1')\n",
    "\n",
    "print('INCREASING DATASET WITH APP350')\n",
    "relevantfiles = ['data/App350/Contact_E_Mail_Address_1stParty.txt',\n",
    "                 'data/App350/Contact_E_Mail_Address_3rdParty.txt']\n",
    "for filepath in relevantfiles:\n",
    "    with open(filepath, encoding='utf8') as f:   #utf8      \n",
    "        text = f.readlines()\n",
    "        text = [k.replace('\\n', '').strip() for k in text]\n",
    "        print('\\t', len(text), 'new sentences found in ', filepath)\n",
    "        for t in text:\n",
    "            dfaux = dfaux.append({'Permission':categoryname, 'Sentence':t, 'Label':labelindex, columnToUse:0},  ignore_index=True)\n",
    "            \n",
    "X0 = dfaux.loc[dfaux[columnToUse] == 0]['Sentence'].tolist()\n",
    "#X0 = X0*2\n",
    "xn0 = len(X0)\n",
    "X1 = dfaux.loc[dfaux[columnToUse] == 1]['Sentence'].tolist()\n",
    "xn1 = len(X1)\n",
    "print('\\t',xn0, 'sentences of class 0; ', xn1, 'sentence for class 1')\n",
    "            \n",
    "            \n",
    "print('REMOVING REPEATED SENTENCES FROM CLASS 1 THAT ALSO BELONG TO CLASS 0')\n",
    "#then check for repeated sentences belonging to both X0 and X1. If it belongs to X0, we remove it from X1.\n",
    "#This happens, sentences are repeated in the dataset as belonging to more than one privacy category, hence we clean it\n",
    "#before letting the classifier learn\n",
    "totalr = 0\n",
    "for s0 in X0:\n",
    "    s0 = s0.strip()\n",
    "    ir = [index for index, s in enumerate(X1) if s.strip()==s0]\n",
    "    ir = sorted(ir, reverse = True) #need to sort in reverse so when deleting an index does not affect the rest of indices\n",
    "    for index in ir:\n",
    "        del X1[index]\n",
    "    totalr += len(ir)\n",
    "print('\\tRemoved', totalr, 'repeated sentences from X1')\n",
    "\n",
    "#subsampling (random) of largest class\n",
    "print('CHECKING THE PROPORTION OF INSTANCES IN EACH CLASS')\n",
    "if(len(X1)>len(X0)*classproportion):\n",
    "    X1 = random.sample(X1, xn0*classproportion)\n",
    "    xn1 = len(X1)\n",
    "    print('\\tclass 1 is larger than 0, applying subsampling')\n",
    "elif(len(X0)>len(X1)*classproportion):\n",
    "    X0 = random.sample(X0, xn1*classproportion)\n",
    "    xn0 = len(X0)\n",
    "    print('\\tclass 0 is larger than 1')\n",
    "print('\\t',xn0, 'sentences of class 0; ', xn1, 'sentence for class 1')\n",
    "X = X0+X1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">Train: 0=2097, 1=4195, Test: 0=111, 1=221\n",
      "CM [[ 97  14]\n",
      " [ 13 208]]\n",
      "F1 0.8778280542986425\n",
      "Acc 0.9186746987951807\n",
      "*Model for  Label3 saved in  models/Label3_new_model.pkl\n"
     ]
    }
   ],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "text_clf = Pipeline([\n",
    "    ('vect', CountVectorizer(ngram_range=ngrams, binary=apply_binary)),\n",
    "    ('tfidf', TfidfTransformer(use_idf=apply_tfidf)),\n",
    "    ('clf', SGDClassifier(loss=svm_loss, penalty='l2',\n",
    "                          alpha=svm_alpha, max_iter=10000, tol=None,  class_weight='balanced')),\n",
    "    ])\n",
    "\n",
    "labels = xn0*[0] + xn1*[1]\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, labels, test_size=0.05, stratify=labels)\n",
    "\n",
    "#print proportion for training and testing\n",
    "X_train=np.array(X_train);X_test=np.array(X_test);y_train=np.array(y_train);y_test=np.array(y_test);\n",
    "train_0, train_1 = len(y_train[y_train==0]), len(y_train[y_train==1])\n",
    "test_0, test_1 = len(y_test[y_test==0]), len(y_test[y_test==1])\n",
    "print('>Train: 0=%d, 1=%d, Test: 0=%d, 1=%d' % (train_0, train_1, test_0, test_1))\n",
    "\n",
    "\n",
    "#train and test\n",
    "text_clf.fit(X_train, y_train)          \n",
    "pred_y = text_clf.predict(X_test)\n",
    "#plot_confusion_matrix(clf, test_X, test_y)\n",
    "cm = confusion_matrix(y_test, pred_y)\n",
    "f1 = f1_score(y_test, pred_y, average=None)[0]\n",
    "acc = np.mean(pred_y == y_test) \n",
    "\n",
    "print('CM', cm)\n",
    "print('F1', f1)\n",
    "print('Acc', acc)\n",
    "\n",
    "#save model\n",
    "pkl_filename = 'models/'+columnToUse+\"_new_model.pkl\"\n",
    "with open(pkl_filename, 'wb') as file:\n",
    "    pickle.dump(text_clf, file)\n",
    "    print('*Model for ', columnToUse, 'saved in ', pkl_filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "### Test without ignoring 'None' sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ORIGINAL DATASET SAMPLING\n",
      "\t 12520 instances to classify,  2208 for class 0,  10312 for class 1\n",
      "REMOVING REPEATED SENTENCES FROM CLASS 1 THAT ALSO BELONG TO CLASS 0\n",
      "\tRemoved 270 repeated sentences from X1\n",
      "CM [[2191   17]\n",
      " [ 498 9544]]\n",
      "F1 0.8948335715744333\n",
      "Acc 0.9579591836734694\n"
     ]
    }
   ],
   "source": [
    "#Load classifier for 'amazon pay' and test it on the entire dataset\n",
    "pkl_filename = 'models/'+columnToUse+\"_new_model.pkl\"\n",
    "with open(pkl_filename, 'rb') as file:\n",
    "    text_clf = pickle.load(file)\n",
    "    \n",
    "#get the data\n",
    "#ignoring None sentences\n",
    "#X = dfaux.loc[dfaux['Permission'] != 'None', 'Sentence']\n",
    "#y = dfaux.loc[dfaux['Permission'] != 'None', columnToUse]\n",
    "X0 = dfaux.loc[dfaux[columnToUse] == 0]['Sentence'].tolist()\n",
    "xn0 = len(X0)\n",
    "X1 = dfaux.loc[dfaux[columnToUse] == 1]['Sentence'].tolist()\n",
    "xn1 = len(X1)\n",
    "print('ORIGINAL DATASET SAMPLING')\n",
    "print('\\t',len(X0+X1), 'instances to classify, ',xn0, 'for class 0, ', xn1, 'for class 1' )\n",
    "print('REMOVING REPEATED SENTENCES FROM CLASS 1 THAT ALSO BELONG TO CLASS 0')\n",
    "#then check for repeated sentences belonging to both X0 and X1. If it belongs to X0, we remove it from X1.\n",
    "#This happens, sentences are repeated in the dataset as belonging to more than one privacy category, hence we clean it\n",
    "#before letting the classifier learn\n",
    "totalr = 0\n",
    "for s0 in X0:\n",
    "    s0 = s0.strip()\n",
    "    ir = [index for index, s in enumerate(X1) if s.strip()==s0]\n",
    "    ir = sorted(ir, reverse = True) #need to sort in reverse so when deleting an index does not affect the rest of indices\n",
    "    for index in ir:\n",
    "        del X1[index]\n",
    "    totalr += len(ir)\n",
    "print('\\tRemoved', totalr, 'repeated sentences from X1')\n",
    "X = X0+X1\n",
    "xn0 = len(X0);xn1 = len(X1)\n",
    "y = xn0*[0] + xn1*[1]\n",
    "\n",
    "#test the data\n",
    "pred_y = text_clf.predict(X)\n",
    "cm = confusion_matrix(y, pred_y)\n",
    "f1 = f1_score(y, pred_y, average=None)[0]\n",
    "acc = np.mean(pred_y == y)\n",
    "\n",
    "print('CM', cm)\n",
    "print('F1', f1)\n",
    "print('Acc', acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "### Test ignoring 'None' sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ORIGINAL DATASET SAMPLING\n",
      "\t 6055 instances to classify,  2208 for class 0,  3847 for class 1\n",
      "REMOVING REPEATED SENTENCES FROM CLASS 1 THAT ALSO BELONG TO CLASS 0\n",
      "\tRemoved 270 repeated sentences from X1\n",
      "CM [[2191   17]\n",
      " [ 288 3289]]\n",
      "F1 0.9349263921484958\n",
      "Acc 0.9472774416594641\n"
     ]
    }
   ],
   "source": [
    "#Load classifier for 'amazon pay' and test it on the entire dataset\n",
    "pkl_filename = 'models/'+columnToUse+\"_new_model.pkl\"\n",
    "with open(pkl_filename, 'rb') as file:\n",
    "    text_clf = pickle.load(file)\n",
    "    \n",
    "#get the data\n",
    "#ignoring None sentences\n",
    "#X = dfaux.loc[dfaux['Permission'] != 'None', 'Sentence']\n",
    "#y = dfaux.loc[dfaux['Permission'] != 'None', columnToUse]\n",
    "X0 = dfaux.loc[(dfaux[columnToUse] == 0)&(dfaux['Permission'] != 'None')]['Sentence'].tolist()\n",
    "xn0 = len(X0)\n",
    "X1 = dfaux.loc[(dfaux[columnToUse] == 1)&(dfaux['Permission'] != 'None')]['Sentence'].tolist()\n",
    "xn1 = len(X1)\n",
    "print('ORIGINAL DATASET SAMPLING')\n",
    "print('\\t',len(X0+X1), 'instances to classify, ',xn0, 'for class 0, ', xn1, 'for class 1' )\n",
    "print('REMOVING REPEATED SENTENCES FROM CLASS 1 THAT ALSO BELONG TO CLASS 0')\n",
    "#then check for repeated sentences belonging to both X0 and X1. If it belongs to X0, we remove it from X1.\n",
    "#This happens, sentences are repeated in the dataset as belonging to more than one privacy category, hence we clean it\n",
    "#before letting the classifier learn\n",
    "totalr = 0\n",
    "for s0 in X0:\n",
    "    s0 = s0.strip()\n",
    "    ir = [index for index, s in enumerate(X1) if s.strip()==s0]\n",
    "    ir = sorted(ir, reverse = True) #need to sort in reverse so when deleting an index does not affect the rest of indices\n",
    "    for index in ir:\n",
    "        del X1[index]\n",
    "    totalr += len(ir)\n",
    "print('\\tRemoved', totalr, 'repeated sentences from X1')\n",
    "X = X0+X1\n",
    "xn0 = len(X0);xn1 = len(X1)\n",
    "y = xn0*[0] + xn1*[1]\n",
    "\n",
    "#test the data\n",
    "pred_y = text_clf.predict(X)\n",
    "cm = confusion_matrix(y, pred_y)\n",
    "f1 = f1_score(y, pred_y, average=None)[0]\n",
    "acc = np.mean(pred_y == y)\n",
    "\n",
    "print('CM', cm)\n",
    "print('F1', f1)\n",
    "print('Acc', acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Classifier for class 'Location Services' - 0.9+"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Location Services  -  4  -  Label4\n"
     ]
    }
   ],
   "source": [
    "categoryname = 'Location Services'\n",
    "labelindex = labelToindex[categoryname]\n",
    "columnToUse = 'Label'+str(labelindex)\n",
    "print(categoryname, ' - ', labelindex, ' - ', columnToUse)\n",
    "\n",
    "classproportion = 4\n",
    "ngrams = (1,3)\n",
    "apply_tfidf = False\n",
    "apply_binary = True\n",
    "svm_loss = 'modified_huber'\n",
    "svm_alpha = 0.0001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ORIGINAL DATASET SAMPLING\n",
      "\t 327 sentences of class 0;  10524 sentence for class 1\n",
      "INCREASING DATASET WITH APP350\n",
      "\t 278 new sentences found in  data/App350/Location_GPS_1stParty.txt\n",
      "\t 69 new sentences found in  data/App350/Location_GPS_3rdParty.txt\n",
      "\t 674 sentences of class 0;  10524 sentence for class 1\n",
      "REMOVING REPEATED SENTENCES FROM CLASS 1 THAT ALSO BELONG TO CLASS 0\n",
      "\tRemoved 110 repeated sentences from X1\n",
      "CHECKING THE PROPORTION OF INSTANCES IN EACH CLASS\n",
      "\tclass 1 is larger than 0, applying subsampling\n",
      "\t 674 sentences of class 0;  2696 sentence for class 1\n"
     ]
    }
   ],
   "source": [
    "dfaux = df\n",
    "\n",
    "X0 = dfaux.loc[dfaux[columnToUse] == 0]['Sentence'].tolist()\n",
    "xn0 = len(X0)\n",
    "X1 = dfaux.loc[dfaux[columnToUse] == 1]['Sentence'].tolist()\n",
    "xn1 = len(X1)\n",
    "\n",
    "print('ORIGINAL DATASET SAMPLING')\n",
    "print('\\t',xn0, 'sentences of class 0; ', xn1, 'sentence for class 1')\n",
    "\n",
    "print('INCREASING DATASET WITH APP350')\n",
    "relevantfiles = ['data/App350/Location_GPS_1stParty.txt',\n",
    "                 'data/App350/Location_GPS_3rdParty.txt']\n",
    "for filepath in relevantfiles:\n",
    "    with open(filepath, encoding='utf8') as f:   #utf8      \n",
    "        text = f.readlines()\n",
    "        text = [k.replace('\\n', '').strip() for k in text]\n",
    "        print('\\t', len(text), 'new sentences found in ', filepath)\n",
    "        for t in text:\n",
    "            dfaux = dfaux.append({'Permission':categoryname, 'Sentence':t, 'Label':labelindex, columnToUse:0},  ignore_index=True)\n",
    "            \n",
    "X0 = dfaux.loc[dfaux[columnToUse] == 0]['Sentence'].tolist()\n",
    "#X0 = X0*2\n",
    "xn0 = len(X0)\n",
    "X1 = dfaux.loc[dfaux[columnToUse] == 1]['Sentence'].tolist()\n",
    "xn1 = len(X1)\n",
    "print('\\t',xn0, 'sentences of class 0; ', xn1, 'sentence for class 1')\n",
    "            \n",
    "            \n",
    "print('REMOVING REPEATED SENTENCES FROM CLASS 1 THAT ALSO BELONG TO CLASS 0')\n",
    "#then check for repeated sentences belonging to both X0 and X1. If it belongs to X0, we remove it from X1.\n",
    "#This happens, sentences are repeated in the dataset as belonging to more than one privacy category, hence we clean it\n",
    "#before letting the classifier learn\n",
    "totalr = 0\n",
    "for s0 in X0:\n",
    "    s0 = s0.strip()\n",
    "    ir = [index for index, s in enumerate(X1) if s.strip()==s0]\n",
    "    ir = sorted(ir, reverse = True) #need to sort in reverse so when deleting an index does not affect the rest of indices\n",
    "    for index in ir:\n",
    "        del X1[index]\n",
    "    totalr += len(ir)\n",
    "print('\\tRemoved', totalr, 'repeated sentences from X1')\n",
    "\n",
    "#subsampling (random) of largest class\n",
    "print('CHECKING THE PROPORTION OF INSTANCES IN EACH CLASS')\n",
    "if(len(X1)>len(X0)*classproportion):\n",
    "    X1 = random.sample(X1, xn0*classproportion)\n",
    "    xn1 = len(X1)\n",
    "    print('\\tclass 1 is larger than 0, applying subsampling')\n",
    "elif(len(X0)>len(X1)*classproportion):\n",
    "    X0 = random.sample(X0, xn1*classproportion)\n",
    "    xn0 = len(X0)\n",
    "    print('\\tclass 0 is larger than 1')\n",
    "print('\\t',xn0, 'sentences of class 0; ', xn1, 'sentence for class 1')\n",
    "X = X0+X1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">Train: 0=640, 1=2561, Test: 0=34, 1=135\n",
      "CM [[ 28   6]\n",
      " [  0 135]]\n",
      "F1 0.9032258064516129\n",
      "Acc 0.9644970414201184\n",
      "*Model for  Label4 saved in  models/Label4_new_model.pkl\n"
     ]
    }
   ],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "text_clf = Pipeline([\n",
    "    ('vect', CountVectorizer(ngram_range=ngrams, binary=apply_binary)),\n",
    "    ('tfidf', TfidfTransformer(use_idf=apply_tfidf)),\n",
    "    ('clf', SGDClassifier(loss=svm_loss, penalty='l2',\n",
    "                          alpha=svm_alpha, max_iter=10000, tol=None,  class_weight='balanced')),\n",
    "    ])\n",
    "\n",
    "labels = xn0*[0] + xn1*[1]\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, labels, test_size=0.05, stratify=labels)\n",
    "\n",
    "#print proportion for training and testing\n",
    "X_train=np.array(X_train);X_test=np.array(X_test);y_train=np.array(y_train);y_test=np.array(y_test);\n",
    "train_0, train_1 = len(y_train[y_train==0]), len(y_train[y_train==1])\n",
    "test_0, test_1 = len(y_test[y_test==0]), len(y_test[y_test==1])\n",
    "print('>Train: 0=%d, 1=%d, Test: 0=%d, 1=%d' % (train_0, train_1, test_0, test_1))\n",
    "\n",
    "\n",
    "#train and test\n",
    "text_clf.fit(X_train, y_train)          \n",
    "pred_y = text_clf.predict(X_test)\n",
    "#plot_confusion_matrix(clf, test_X, test_y)\n",
    "cm = confusion_matrix(y_test, pred_y)\n",
    "f1 = f1_score(y_test, pred_y, average=None)[0]\n",
    "acc = np.mean(pred_y == y_test) \n",
    "\n",
    "print('CM', cm)\n",
    "print('F1', f1)\n",
    "print('Acc', acc)\n",
    "\n",
    "#save model\n",
    "pkl_filename = 'models/'+columnToUse+\"_new_model.pkl\"\n",
    "with open(pkl_filename, 'wb') as file:\n",
    "    pickle.dump(text_clf, file)\n",
    "    print('*Model for ', columnToUse, 'saved in ', pkl_filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "### Test without ignoring 'None' sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ORIGINAL DATASET SAMPLING\n",
      "\t 11198 instances to classify,  674 for class 0,  10524 for class 1\n",
      "REMOVING REPEATED SENTENCES FROM CLASS 1 THAT ALSO BELONG TO CLASS 0\n",
      "\tRemoved 110 repeated sentences from X1\n",
      "CM [[  668     6]\n",
      " [   45 10369]]\n",
      "F1 0.9632299927901946\n",
      "Acc 0.9954004329004329\n"
     ]
    }
   ],
   "source": [
    "#Load classifier for 'amazon pay' and test it on the entire dataset\n",
    "pkl_filename = 'models/'+columnToUse+\"_new_model.pkl\"\n",
    "with open(pkl_filename, 'rb') as file:\n",
    "    text_clf = pickle.load(file)\n",
    "    \n",
    "#get the data\n",
    "#ignoring None sentences\n",
    "#X = dfaux.loc[dfaux['Permission'] != 'None', 'Sentence']\n",
    "#y = dfaux.loc[dfaux['Permission'] != 'None', columnToUse]\n",
    "X0 = dfaux.loc[dfaux[columnToUse] == 0]['Sentence'].tolist()\n",
    "xn0 = len(X0)\n",
    "X1 = dfaux.loc[dfaux[columnToUse] == 1]['Sentence'].tolist()\n",
    "xn1 = len(X1)\n",
    "print('ORIGINAL DATASET SAMPLING')\n",
    "print('\\t',len(X0+X1), 'instances to classify, ',xn0, 'for class 0, ', xn1, 'for class 1' )\n",
    "print('REMOVING REPEATED SENTENCES FROM CLASS 1 THAT ALSO BELONG TO CLASS 0')\n",
    "#then check for repeated sentences belonging to both X0 and X1. If it belongs to X0, we remove it from X1.\n",
    "#This happens, sentences are repeated in the dataset as belonging to more than one privacy category, hence we clean it\n",
    "#before letting the classifier learn\n",
    "totalr = 0\n",
    "for s0 in X0:\n",
    "    s0 = s0.strip()\n",
    "    ir = [index for index, s in enumerate(X1) if s.strip()==s0]\n",
    "    ir = sorted(ir, reverse = True) #need to sort in reverse so when deleting an index does not affect the rest of indices\n",
    "    for index in ir:\n",
    "        del X1[index]\n",
    "    totalr += len(ir)\n",
    "print('\\tRemoved', totalr, 'repeated sentences from X1')\n",
    "X = X0+X1\n",
    "xn0 = len(X0);xn1 = len(X1)\n",
    "y = xn0*[0] + xn1*[1]\n",
    "\n",
    "#test the data\n",
    "pred_y = text_clf.predict(X)\n",
    "cm = confusion_matrix(y, pred_y)\n",
    "f1 = f1_score(y, pred_y, average=None)[0]\n",
    "acc = np.mean(pred_y == y)\n",
    "\n",
    "print('CM', cm)\n",
    "print('F1', f1)\n",
    "print('Acc', acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "### Test ignoring 'None' sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ORIGINAL DATASET SAMPLING\n",
      "\t 4745 instances to classify,  674 for class 0,  4071 for class 1\n",
      "REMOVING REPEATED SENTENCES FROM CLASS 1 THAT ALSO BELONG TO CLASS 0\n",
      "\tRemoved 109 repeated sentences from X1\n",
      "CM [[ 668    6]\n",
      " [  22 3940]]\n",
      "F1 0.9794721407624634\n",
      "Acc 0.993960310612597\n"
     ]
    }
   ],
   "source": [
    "#Load classifier for 'amazon pay' and test it on the entire dataset\n",
    "pkl_filename = 'models/'+columnToUse+\"_new_model.pkl\"\n",
    "with open(pkl_filename, 'rb') as file:\n",
    "    text_clf = pickle.load(file)\n",
    "    \n",
    "#get the data\n",
    "#ignoring None sentences\n",
    "#X = dfaux.loc[dfaux['Permission'] != 'None', 'Sentence']\n",
    "#y = dfaux.loc[dfaux['Permission'] != 'None', columnToUse]\n",
    "X0 = dfaux.loc[(dfaux[columnToUse] == 0)&(dfaux['Permission'] != 'None')]['Sentence'].tolist()\n",
    "xn0 = len(X0)\n",
    "X1 = dfaux.loc[(dfaux[columnToUse] == 1)&(dfaux['Permission'] != 'None')]['Sentence'].tolist()\n",
    "xn1 = len(X1)\n",
    "print('ORIGINAL DATASET SAMPLING')\n",
    "print('\\t',len(X0+X1), 'instances to classify, ',xn0, 'for class 0, ', xn1, 'for class 1' )\n",
    "print('REMOVING REPEATED SENTENCES FROM CLASS 1 THAT ALSO BELONG TO CLASS 0')\n",
    "#then check for repeated sentences belonging to both X0 and X1. If it belongs to X0, we remove it from X1.\n",
    "#This happens, sentences are repeated in the dataset as belonging to more than one privacy category, hence we clean it\n",
    "#before letting the classifier learn\n",
    "totalr = 0\n",
    "for s0 in X0:\n",
    "    s0 = s0.strip()\n",
    "    ir = [index for index, s in enumerate(X1) if s.strip()==s0]\n",
    "    ir = sorted(ir, reverse = True) #need to sort in reverse so when deleting an index does not affect the rest of indices\n",
    "    for index in ir:\n",
    "        del X1[index]\n",
    "    totalr += len(ir)\n",
    "print('\\tRemoved', totalr, 'repeated sentences from X1')\n",
    "X = X0+X1\n",
    "xn0 = len(X0);xn1 = len(X1)\n",
    "y = xn0*[0] + xn1*[1]\n",
    "\n",
    "#test the data\n",
    "pred_y = text_clf.predict(X)\n",
    "cm = confusion_matrix(y, pred_y)\n",
    "f1 = f1_score(y, pred_y, average=None)[0]\n",
    "acc = np.mean(pred_y == y)\n",
    "\n",
    "print('CM', cm)\n",
    "print('F1', f1)\n",
    "print('Acc', acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Classifier for class 'Mobile Number' - 0.9+"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mobile Number  -  5  -  Label5\n"
     ]
    }
   ],
   "source": [
    "categoryname = 'Mobile Number'\n",
    "labelindex = labelToindex[categoryname]\n",
    "columnToUse = 'Label'+str(labelindex)\n",
    "print(categoryname, ' - ', labelindex, ' - ', columnToUse)\n",
    "\n",
    "classproportion = 2\n",
    "ngrams = (1,3)\n",
    "apply_tfidf = False\n",
    "apply_binary = True\n",
    "svm_loss = 'modified_huber'\n",
    "svm_alpha = 0.0001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ORIGINAL DATASET SAMPLING\n",
      "\t 97 sentences of class 0;  10447 sentence for class 1\n",
      "INCREASING DATASET WITH APP350\n",
      "\t 772 new sentences found in  data/App350/Contact_Phone_Number_1stParty.txt\n",
      "\t 72 new sentences found in  data/App350/Contact_Phone_Number_3rdParty.txt\n",
      "\t 1882 sentences of class 0;  10447 sentence for class 1\n",
      "REMOVING REPEATED SENTENCES FROM CLASS 1 THAT ALSO BELONG TO CLASS 0\n",
      "\tRemoved 197 repeated sentences from X1\n",
      "CHECKING THE PROPORTION OF INSTANCES IN EACH CLASS\n",
      "\tclass 1 is larger than 0, applying subsampling\n",
      "\t 1882 sentences of class 0;  3764 sentence for class 1\n",
      ">Train: 0=1788, 1=3575, Test: 0=94, 1=189\n",
      "CM [[ 94   0]\n",
      " [  6 183]]\n",
      "F1 0.9690721649484536\n",
      "Acc 0.9787985865724381\n",
      "*Model for  Label5 saved in  models/Label5_new_model.pkl\n"
     ]
    }
   ],
   "source": [
    "dfaux = df\n",
    "\n",
    "X0 = dfaux.loc[dfaux[columnToUse] == 0]['Sentence'].tolist()\n",
    "xn0 = len(X0)\n",
    "X1 = dfaux.loc[dfaux[columnToUse] == 1]['Sentence'].tolist()\n",
    "xn1 = len(X1)\n",
    "\n",
    "print('ORIGINAL DATASET SAMPLING')\n",
    "print('\\t',xn0, 'sentences of class 0; ', xn1, 'sentence for class 1')\n",
    "\n",
    "print('INCREASING DATASET WITH APP350')\n",
    "relevantfiles = ['data/App350/Contact_Phone_Number_1stParty.txt',\n",
    "                 'data/App350/Contact_Phone_Number_3rdParty.txt']\n",
    "for filepath in relevantfiles:\n",
    "    with open(filepath, encoding='utf8') as f:   #utf8      \n",
    "        text = f.readlines()\n",
    "        text = [k.replace('\\n', '').strip() for k in text]\n",
    "        print('\\t', len(text), 'new sentences found in ', filepath)\n",
    "        for t in text:\n",
    "            dfaux = dfaux.append({'Permission':categoryname, 'Sentence':t, 'Label':labelindex, columnToUse:0},  ignore_index=True)\n",
    "            \n",
    "X0 = dfaux.loc[dfaux[columnToUse] == 0]['Sentence'].tolist()\n",
    "X0 = X0*2\n",
    "xn0 = len(X0)\n",
    "X1 = dfaux.loc[dfaux[columnToUse] == 1]['Sentence'].tolist()\n",
    "xn1 = len(X1)\n",
    "print('\\t',xn0, 'sentences of class 0; ', xn1, 'sentence for class 1')\n",
    "            \n",
    "            \n",
    "print('REMOVING REPEATED SENTENCES FROM CLASS 1 THAT ALSO BELONG TO CLASS 0')\n",
    "#then check for repeated sentences belonging to both X0 and X1. If it belongs to X0, we remove it from X1.\n",
    "#This happens, sentences are repeated in the dataset as belonging to more than one privacy category, hence we clean it\n",
    "#before letting the classifier learn\n",
    "totalr = 0\n",
    "for s0 in X0:\n",
    "    s0 = s0.strip()\n",
    "    ir = [index for index, s in enumerate(X1) if s.strip()==s0]\n",
    "    ir = sorted(ir, reverse = True) #need to sort in reverse so when deleting an index does not affect the rest of indices\n",
    "    for index in ir:\n",
    "        del X1[index]\n",
    "    totalr += len(ir)\n",
    "print('\\tRemoved', totalr, 'repeated sentences from X1')\n",
    "\n",
    "#subsampling (random) of largest class\n",
    "print('CHECKING THE PROPORTION OF INSTANCES IN EACH CLASS')\n",
    "if(len(X1)>len(X0)*classproportion):\n",
    "    X1 = random.sample(X1, xn0*classproportion)\n",
    "    xn1 = len(X1)\n",
    "    print('\\tclass 1 is larger than 0, applying subsampling')\n",
    "elif(len(X0)>len(X1)*classproportion):\n",
    "    X0 = random.sample(X0, xn1*classproportion)\n",
    "    xn0 = len(X0)\n",
    "    print('\\tclass 0 is larger than 1')\n",
    "print('\\t',xn0, 'sentences of class 0; ', xn1, 'sentence for class 1')\n",
    "X = X0+X1\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "text_clf = Pipeline([\n",
    "    ('vect', CountVectorizer(ngram_range=ngrams, binary=apply_binary)),\n",
    "    ('tfidf', TfidfTransformer(use_idf=apply_tfidf)),\n",
    "    ('clf', SGDClassifier(loss=svm_loss, penalty='l2',\n",
    "                          alpha=svm_alpha, max_iter=10000, tol=None,  class_weight='balanced')),\n",
    "    ])\n",
    "\n",
    "labels = xn0*[0] + xn1*[1]\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, labels, test_size=0.05, stratify=labels)\n",
    "\n",
    "#print proportion for training and testing\n",
    "X_train=np.array(X_train);X_test=np.array(X_test);y_train=np.array(y_train);y_test=np.array(y_test);\n",
    "train_0, train_1 = len(y_train[y_train==0]), len(y_train[y_train==1])\n",
    "test_0, test_1 = len(y_test[y_test==0]), len(y_test[y_test==1])\n",
    "print('>Train: 0=%d, 1=%d, Test: 0=%d, 1=%d' % (train_0, train_1, test_0, test_1))\n",
    "\n",
    "\n",
    "#train and test\n",
    "text_clf.fit(X_train, y_train)          \n",
    "pred_y = text_clf.predict(X_test)\n",
    "#plot_confusion_matrix(clf, test_X, test_y)\n",
    "cm = confusion_matrix(y_test, pred_y)\n",
    "f1 = f1_score(y_test, pred_y, average=None)[0]\n",
    "acc = np.mean(pred_y == y_test) \n",
    "\n",
    "print('CM', cm)\n",
    "print('F1', f1)\n",
    "print('Acc', acc)\n",
    "\n",
    "#save model\n",
    "pkl_filename = 'models/'+columnToUse+\"_new_model.pkl\"\n",
    "with open(pkl_filename, 'wb') as file:\n",
    "    pickle.dump(text_clf, file)\n",
    "    print('*Model for ', columnToUse, 'saved in ', pkl_filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "### Without ignoring None sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ORIGINAL DATASET SAMPLING\n",
      "\t 11388 instances to classify,  941 for class 0,  10447 for class 1\n",
      "REMOVING REPEATED SENTENCES FROM CLASS 1 THAT ALSO BELONG TO CLASS 0\n",
      "\tRemoved 197 repeated sentences from X1\n",
      "CM [[ 940    1]\n",
      " [ 270 9980]]\n",
      "F1 0.8740120874012087\n",
      "Acc 0.9757841122330444\n"
     ]
    }
   ],
   "source": [
    "#Load classifier for 'amazon pay' and test it on the entire dataset\n",
    "pkl_filename = 'models/'+columnToUse+\"_new_model.pkl\"\n",
    "with open(pkl_filename, 'rb') as file:\n",
    "    text_clf = pickle.load(file)\n",
    "    \n",
    "#get the data\n",
    "#ignoring None sentences\n",
    "#X = dfaux.loc[dfaux['Permission'] != 'None', 'Sentence']\n",
    "#y = dfaux.loc[dfaux['Permission'] != 'None', columnToUse]\n",
    "X0 = dfaux.loc[dfaux[columnToUse] == 0]['Sentence'].tolist()\n",
    "xn0 = len(X0)\n",
    "X1 = dfaux.loc[dfaux[columnToUse] == 1]['Sentence'].tolist()\n",
    "xn1 = len(X1)\n",
    "print('ORIGINAL DATASET SAMPLING')\n",
    "print('\\t',len(X0+X1), 'instances to classify, ',xn0, 'for class 0, ', xn1, 'for class 1' )\n",
    "print('REMOVING REPEATED SENTENCES FROM CLASS 1 THAT ALSO BELONG TO CLASS 0')\n",
    "#then check for repeated sentences belonging to both X0 and X1. If it belongs to X0, we remove it from X1.\n",
    "#This happens, sentences are repeated in the dataset as belonging to more than one privacy category, hence we clean it\n",
    "#before letting the classifier learn\n",
    "totalr = 0\n",
    "for s0 in X0:\n",
    "    s0 = s0.strip()\n",
    "    ir = [index for index, s in enumerate(X1) if s.strip()==s0]\n",
    "    ir = sorted(ir, reverse = True) #need to sort in reverse so when deleting an index does not affect the rest of indices\n",
    "    for index in ir:\n",
    "        del X1[index]\n",
    "    totalr += len(ir)\n",
    "print('\\tRemoved', totalr, 'repeated sentences from X1')\n",
    "X = X0+X1\n",
    "xn0 = len(X0);xn1 = len(X1)\n",
    "y = xn0*[0] + xn1*[1]\n",
    "\n",
    "#test the data\n",
    "pred_y = text_clf.predict(X)\n",
    "cm = confusion_matrix(y, pred_y)\n",
    "f1 = f1_score(y, pred_y, average=None)[0]\n",
    "acc = np.mean(pred_y == y)\n",
    "\n",
    "print('CM', cm)\n",
    "print('F1', f1)\n",
    "print('Acc', acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "### Ignoring None sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ORIGINAL DATASET SAMPLING\n",
      "\t 4923 instances to classify,  941 for class 0,  3982 for class 1\n",
      "REMOVING REPEATED SENTENCES FROM CLASS 1 THAT ALSO BELONG TO CLASS 0\n",
      "\tRemoved 193 repeated sentences from X1\n",
      "CM [[ 940    1]\n",
      " [ 225 3564]]\n",
      "F1 0.8926875593542261\n",
      "Acc 0.9522198731501057\n"
     ]
    }
   ],
   "source": [
    "#Load classifier for 'amazon pay' and test it on the entire dataset\n",
    "pkl_filename = 'models/'+columnToUse+\"_new_model.pkl\"\n",
    "with open(pkl_filename, 'rb') as file:\n",
    "    text_clf = pickle.load(file)\n",
    "    \n",
    "#get the data\n",
    "#ignoring None sentences\n",
    "#X = dfaux.loc[dfaux['Permission'] != 'None', 'Sentence']\n",
    "#y = dfaux.loc[dfaux['Permission'] != 'None', columnToUse]\n",
    "X0 = dfaux.loc[(dfaux[columnToUse] == 0)&(dfaux['Permission'] != 'None')]['Sentence'].tolist()\n",
    "xn0 = len(X0)\n",
    "X1 = dfaux.loc[(dfaux[columnToUse] == 1)&(dfaux['Permission'] != 'None')]['Sentence'].tolist()\n",
    "xn1 = len(X1)\n",
    "print('ORIGINAL DATASET SAMPLING')\n",
    "print('\\t',len(X0+X1), 'instances to classify, ',xn0, 'for class 0, ', xn1, 'for class 1' )\n",
    "print('REMOVING REPEATED SENTENCES FROM CLASS 1 THAT ALSO BELONG TO CLASS 0')\n",
    "#then check for repeated sentences belonging to both X0 and X1. If it belongs to X0, we remove it from X1.\n",
    "#This happens, sentences are repeated in the dataset as belonging to more than one privacy category, hence we clean it\n",
    "#before letting the classifier learn\n",
    "totalr = 0\n",
    "for s0 in X0:\n",
    "    s0 = s0.strip()\n",
    "    ir = [index for index, s in enumerate(X1) if s.strip()==s0]\n",
    "    ir = sorted(ir, reverse = True) #need to sort in reverse so when deleting an index does not affect the rest of indices\n",
    "    for index in ir:\n",
    "        del X1[index]\n",
    "    totalr += len(ir)\n",
    "print('\\tRemoved', totalr, 'repeated sentences from X1')\n",
    "X = X0+X1\n",
    "xn0 = len(X0);xn1 = len(X1)\n",
    "y = xn0*[0] + xn1*[1]\n",
    "\n",
    "#test the data\n",
    "pred_y = text_clf.predict(X)\n",
    "cm = confusion_matrix(y, pred_y)\n",
    "f1 = f1_score(y, pred_y, average=None)[0]\n",
    "acc = np.mean(pred_y == y)\n",
    "\n",
    "print('CM', cm)\n",
    "print('F1', f1)\n",
    "print('Acc', acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Classifier for class 'Name' - 0.9+"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Name  -  6  -  Label6\n"
     ]
    }
   ],
   "source": [
    "categoryname = 'Name'\n",
    "labelindex = labelToindex[categoryname]\n",
    "columnToUse = 'Label'+str(labelindex)\n",
    "print(categoryname, ' - ', labelindex, ' - ', columnToUse)\n",
    "\n",
    "classproportion = 5\n",
    "ngrams = (1,3)\n",
    "apply_tfidf = False\n",
    "apply_binary = True\n",
    "svm_loss = 'modified_huber'\n",
    "svm_alpha = 0.0001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ORIGINAL DATASET SAMPLING\n",
      "\t 424 sentences of class 0;  10120 sentence for class 1\n",
      "REMOVING REPEATED SENTENCES FROM CLASS 1 THAT ALSO BELONG TO CLASS 0\n",
      "\tRemoved 334 repeated sentences from X1\n",
      "CHECKING THE PROPORTION OF INSTANCES IN EACH CLASS\n",
      "\tclass 1 is larger than 0, applying subsampling\n",
      "\t 424 sentences of class 0;  2120 sentence for class 1\n",
      ">Train: 0=403, 1=2013, Test: 0=21, 1=107\n",
      "CM [[ 17   4]\n",
      " [  1 106]]\n",
      "F1 0.8717948717948718\n",
      "Acc 0.9609375\n",
      "*Model for  Label6 saved in  models/Label6_new_model.pkl\n"
     ]
    }
   ],
   "source": [
    "dfaux = df\n",
    "\n",
    "X0 = dfaux.loc[dfaux[columnToUse] == 0]['Sentence'].tolist()\n",
    "xn0 = len(X0)\n",
    "X1 = dfaux.loc[dfaux[columnToUse] == 1]['Sentence'].tolist()\n",
    "xn1 = len(X1)\n",
    "\n",
    "print('ORIGINAL DATASET SAMPLING')\n",
    "print('\\t',xn0, 'sentences of class 0; ', xn1, 'sentence for class 1')\n",
    "            \n",
    "            \n",
    "print('REMOVING REPEATED SENTENCES FROM CLASS 1 THAT ALSO BELONG TO CLASS 0')\n",
    "#then check for repeated sentences belonging to both X0 and X1. If it belongs to X0, we remove it from X1.\n",
    "#This happens, sentences are repeated in the dataset as belonging to more than one privacy category, hence we clean it\n",
    "#before letting the classifier learn\n",
    "totalr = 0\n",
    "for s0 in X0:\n",
    "    s0 = s0.strip()\n",
    "    ir = [index for index, s in enumerate(X1) if s.strip()==s0]\n",
    "    ir = sorted(ir, reverse = True) #need to sort in reverse so when deleting an index does not affect the rest of indices\n",
    "    for index in ir:\n",
    "        del X1[index]\n",
    "    totalr += len(ir)\n",
    "print('\\tRemoved', totalr, 'repeated sentences from X1')\n",
    "\n",
    "#subsampling (random) of largest class\n",
    "print('CHECKING THE PROPORTION OF INSTANCES IN EACH CLASS')\n",
    "if(len(X1)>len(X0)*classproportion):\n",
    "    X1 = random.sample(X1, xn0*classproportion)\n",
    "    xn1 = len(X1)\n",
    "    print('\\tclass 1 is larger than 0, applying subsampling')\n",
    "elif(len(X0)>len(X1)*classproportion):\n",
    "    X0 = random.sample(X0, xn1*classproportion)\n",
    "    xn0 = len(X0)\n",
    "    print('\\tclass 0 is larger than 1')\n",
    "print('\\t',xn0, 'sentences of class 0; ', xn1, 'sentence for class 1')\n",
    "X = X0+X1\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "text_clf = Pipeline([\n",
    "    ('vect', CountVectorizer(ngram_range=ngrams, binary=apply_binary)),\n",
    "    ('tfidf', TfidfTransformer(use_idf=apply_tfidf)),\n",
    "    ('clf', SGDClassifier(loss=svm_loss, penalty='l2',\n",
    "                          alpha=svm_alpha, max_iter=10000, tol=None,  class_weight='balanced')),\n",
    "    ])\n",
    "\n",
    "labels = xn0*[0] + xn1*[1]\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, labels, test_size=0.05, stratify=labels)\n",
    "\n",
    "#print proportion for training and testing\n",
    "X_train=np.array(X_train);X_test=np.array(X_test);y_train=np.array(y_train);y_test=np.array(y_test);\n",
    "train_0, train_1 = len(y_train[y_train==0]), len(y_train[y_train==1])\n",
    "test_0, test_1 = len(y_test[y_test==0]), len(y_test[y_test==1])\n",
    "print('>Train: 0=%d, 1=%d, Test: 0=%d, 1=%d' % (train_0, train_1, test_0, test_1))\n",
    "\n",
    "\n",
    "#train and test\n",
    "text_clf.fit(X_train, y_train)          \n",
    "pred_y = text_clf.predict(X_test)\n",
    "#plot_confusion_matrix(clf, test_X, test_y)\n",
    "cm = confusion_matrix(y_test, pred_y)\n",
    "f1 = f1_score(y_test, pred_y, average=None)[0]\n",
    "acc = np.mean(pred_y == y_test) \n",
    "\n",
    "print('CM', cm)\n",
    "print('F1', f1)\n",
    "print('Acc', acc)\n",
    "\n",
    "#save model\n",
    "pkl_filename = 'models/'+columnToUse+\"_new_model.pkl\"\n",
    "with open(pkl_filename, 'wb') as file:\n",
    "    pickle.dump(text_clf, file)\n",
    "    print('*Model for ', columnToUse, 'saved in ', pkl_filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "### Without ignoring None sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ORIGINAL DATASET SAMPLING\n",
      "\t 10544 instances to classify,  424 for class 0,  10120 for class 1\n",
      "REMOVING REPEATED SENTENCES FROM CLASS 1 THAT ALSO BELONG TO CLASS 0\n",
      "\tRemoved 334 repeated sentences from X1\n",
      "CM [[ 420    4]\n",
      " [ 148 9638]]\n",
      "F1 0.8467741935483871\n",
      "Acc 0.9851126346718903\n"
     ]
    }
   ],
   "source": [
    "#Load classifier for 'amazon pay' and test it on the entire dataset\n",
    "pkl_filename = 'models/'+columnToUse+\"_new_model.pkl\"\n",
    "with open(pkl_filename, 'rb') as file:\n",
    "    text_clf = pickle.load(file)\n",
    "    \n",
    "#get the data\n",
    "#ignoring None sentences\n",
    "X0 = dfaux.loc[dfaux[columnToUse] == 0]['Sentence'].tolist()\n",
    "xn0 = len(X0)\n",
    "X1 = dfaux.loc[dfaux[columnToUse] == 1]['Sentence'].tolist()\n",
    "xn1 = len(X1)\n",
    "print('ORIGINAL DATASET SAMPLING')\n",
    "print('\\t',len(X0+X1), 'instances to classify, ',xn0, 'for class 0, ', xn1, 'for class 1' )\n",
    "print('REMOVING REPEATED SENTENCES FROM CLASS 1 THAT ALSO BELONG TO CLASS 0')\n",
    "#then check for repeated sentences belonging to both X0 and X1. If it belongs to X0, we remove it from X1.\n",
    "#This happens, sentences are repeated in the dataset as belonging to more than one privacy category, hence we clean it\n",
    "#before letting the classifier learn\n",
    "totalr = 0\n",
    "for s0 in X0:\n",
    "    s0 = s0.strip()\n",
    "    ir = [index for index, s in enumerate(X1) if s.strip()==s0]\n",
    "    ir = sorted(ir, reverse = True) #need to sort in reverse so when deleting an index does not affect the rest of indices\n",
    "    for index in ir:\n",
    "        del X1[index]\n",
    "    totalr += len(ir)\n",
    "print('\\tRemoved', totalr, 'repeated sentences from X1')\n",
    "X = X0+X1\n",
    "xn0 = len(X0);xn1 = len(X1)\n",
    "y = xn0*[0] + xn1*[1]\n",
    "\n",
    "#test the data\n",
    "pred_y = text_clf.predict(X)\n",
    "cm = confusion_matrix(y, pred_y)\n",
    "f1 = f1_score(y, pred_y, average=None)[0]\n",
    "acc = np.mean(pred_y == y)\n",
    "\n",
    "print('CM', cm)\n",
    "print('F1', f1)\n",
    "print('Acc', acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "### Ignoring None sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ORIGINAL DATASET SAMPLING\n",
      "\t 4079 instances to classify,  424 for class 0,  3655 for class 1\n",
      "REMOVING REPEATED SENTENCES FROM CLASS 1 THAT ALSO BELONG TO CLASS 0\n",
      "\tRemoved 330 repeated sentences from X1\n",
      "CM [[ 420    4]\n",
      " [ 133 3192]]\n",
      "F1 0.8597748208802457\n",
      "Acc 0.9634569218458255\n"
     ]
    }
   ],
   "source": [
    "#Load classifier for 'amazon pay' and test it on the entire dataset\n",
    "pkl_filename = 'models/'+columnToUse+\"_new_model.pkl\"\n",
    "with open(pkl_filename, 'rb') as file:\n",
    "    text_clf = pickle.load(file)\n",
    "    \n",
    "#get the data\n",
    "#ignoring None sentences\n",
    "X0 = dfaux.loc[(dfaux[columnToUse] == 0)&(dfaux['Permission'] != 'None')]['Sentence'].tolist()\n",
    "xn0 = len(X0)\n",
    "X1 = dfaux.loc[(dfaux[columnToUse] == 1)&(dfaux['Permission'] != 'None')]['Sentence'].tolist()\n",
    "xn1 = len(X1)\n",
    "print('ORIGINAL DATASET SAMPLING')\n",
    "print('\\t',len(X0+X1), 'instances to classify, ',xn0, 'for class 0, ', xn1, 'for class 1' )\n",
    "print('REMOVING REPEATED SENTENCES FROM CLASS 1 THAT ALSO BELONG TO CLASS 0')\n",
    "#then check for repeated sentences belonging to both X0 and X1. If it belongs to X0, we remove it from X1.\n",
    "#This happens, sentences are repeated in the dataset as belonging to more than one privacy category, hence we clean it\n",
    "#before letting the classifier learn\n",
    "totalr = 0\n",
    "for s0 in X0:\n",
    "    s0 = s0.strip()\n",
    "    ir = [index for index, s in enumerate(X1) if s.strip()==s0]\n",
    "    ir = sorted(ir, reverse = True) #need to sort in reverse so when deleting an index does not affect the rest of indices\n",
    "    for index in ir:\n",
    "        del X1[index]\n",
    "    totalr += len(ir)\n",
    "print('\\tRemoved', totalr, 'repeated sentences from X1')\n",
    "X = X0+X1\n",
    "xn0 = len(X0);xn1 = len(X1)\n",
    "y = xn0*[0] + xn1*[1]\n",
    "\n",
    "#test the data\n",
    "pred_y = text_clf.predict(X)\n",
    "cm = confusion_matrix(y, pred_y)\n",
    "f1 = f1_score(y, pred_y, average=None)[0]\n",
    "acc = np.mean(pred_y == y)\n",
    "\n",
    "print('CM', cm)\n",
    "print('F1', f1)\n",
    "print('Acc', acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Classifier for class 'Personal Information' - 99+"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Personal Information  -  7  -  Label7\n"
     ]
    }
   ],
   "source": [
    "categoryname = 'Personal Information'\n",
    "labelindex = labelToindex[categoryname]\n",
    "columnToUse = 'Label'+str(labelindex)\n",
    "print(categoryname, ' - ', labelindex, ' - ', columnToUse)\n",
    "\n",
    "classproportion = 1\n",
    "ngrams = (1,3)\n",
    "apply_tfidf = False\n",
    "apply_binary = True\n",
    "svm_loss = 'modified_huber'\n",
    "svm_alpha = 0.0001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ORIGINAL DATASET SAMPLING\n",
      "\t 2367 sentences of class 0;  8484 sentence for class 1\n",
      "REMOVING REPEATED SENTENCES FROM CLASS 1 THAT ALSO BELONG TO CLASS 0\n",
      "\tRemoved 357 repeated sentences from X1\n",
      "CHECKING THE PROPORTION OF INSTANCES IN EACH CLASS\n",
      "\tclass 1 is larger than 0, applying subsampling\n",
      "\t 2367 sentences of class 0;  2367 sentence for class 1\n",
      ">Train: 0=2249, 1=2248, Test: 0=118, 1=119\n",
      "CM [[118   0]\n",
      " [  1 118]]\n",
      "F1 0.9957805907172996\n",
      "Acc 0.9957805907172996\n",
      "*Model for  Label7 saved in  models/Label7_new_model.pkl\n"
     ]
    }
   ],
   "source": [
    "dfaux = df\n",
    "\n",
    "X0 = dfaux.loc[dfaux[columnToUse] == 0]['Sentence'].tolist()\n",
    "xn0 = len(X0)\n",
    "X1 = dfaux.loc[dfaux[columnToUse] == 1]['Sentence'].tolist()\n",
    "xn1 = len(X1)\n",
    "\n",
    "print('ORIGINAL DATASET SAMPLING')\n",
    "print('\\t',xn0, 'sentences of class 0; ', xn1, 'sentence for class 1')\n",
    "            \n",
    "            \n",
    "print('REMOVING REPEATED SENTENCES FROM CLASS 1 THAT ALSO BELONG TO CLASS 0')\n",
    "#then check for repeated sentences belonging to both X0 and X1. If it belongs to X0, we remove it from X1.\n",
    "#This happens, sentences are repeated in the dataset as belonging to more than one privacy category, hence we clean it\n",
    "#before letting the classifier learn\n",
    "totalr = 0\n",
    "for s0 in X0:\n",
    "    s0 = s0.strip()\n",
    "    ir = [index for index, s in enumerate(X1) if s.strip()==s0]\n",
    "    ir = sorted(ir, reverse = True) #need to sort in reverse so when deleting an index does not affect the rest of indices\n",
    "    for index in ir:\n",
    "        del X1[index]\n",
    "    totalr += len(ir)\n",
    "print('\\tRemoved', totalr, 'repeated sentences from X1')\n",
    "\n",
    "#subsampling (random) of largest class\n",
    "print('CHECKING THE PROPORTION OF INSTANCES IN EACH CLASS')\n",
    "if(len(X1)>len(X0)*classproportion):\n",
    "    X1 = random.sample(X1, xn0*classproportion)\n",
    "    xn1 = len(X1)\n",
    "    print('\\tclass 1 is larger than 0, applying subsampling')\n",
    "elif(len(X0)>len(X1)*classproportion):\n",
    "    X0 = random.sample(X0, xn1*classproportion)\n",
    "    xn0 = len(X0)\n",
    "    print('\\tclass 0 is larger than 1')\n",
    "print('\\t',xn0, 'sentences of class 0; ', xn1, 'sentence for class 1')\n",
    "X = X0+X1\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "text_clf = Pipeline([\n",
    "    ('vect', CountVectorizer(ngram_range=ngrams, binary=apply_binary)),\n",
    "    ('tfidf', TfidfTransformer(use_idf=apply_tfidf)),\n",
    "    ('clf', SGDClassifier(loss=svm_loss, penalty='l2',\n",
    "                          alpha=svm_alpha, max_iter=10000, tol=None,  class_weight='balanced')),\n",
    "    ])\n",
    "\n",
    "labels = xn0*[0] + xn1*[1]\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, labels, test_size=0.05, stratify=labels)\n",
    "\n",
    "#print proportion for training and testing\n",
    "X_train=np.array(X_train);X_test=np.array(X_test);y_train=np.array(y_train);y_test=np.array(y_test);\n",
    "train_0, train_1 = len(y_train[y_train==0]), len(y_train[y_train==1])\n",
    "test_0, test_1 = len(y_test[y_test==0]), len(y_test[y_test==1])\n",
    "print('>Train: 0=%d, 1=%d, Test: 0=%d, 1=%d' % (train_0, train_1, test_0, test_1))\n",
    "\n",
    "\n",
    "#train and test\n",
    "text_clf.fit(X_train, y_train)          \n",
    "pred_y = text_clf.predict(X_test)\n",
    "#plot_confusion_matrix(clf, test_X, test_y)\n",
    "cm = confusion_matrix(y_test, pred_y)\n",
    "f1 = f1_score(y_test, pred_y, average=None)[0]\n",
    "acc = np.mean(pred_y == y_test) \n",
    "\n",
    "print('CM', cm)\n",
    "print('F1', f1)\n",
    "print('Acc', acc)\n",
    "\n",
    "#save model\n",
    "pkl_filename = 'models/'+columnToUse+\"_new_model.pkl\"\n",
    "with open(pkl_filename, 'wb') as file:\n",
    "    pickle.dump(text_clf, file)\n",
    "    print('*Model for ', columnToUse, 'saved in ', pkl_filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "### Without ignoring None sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ORIGINAL DATASET SAMPLING\n",
      "\t 10851 instances to classify,  2367 for class 0,  8484 for class 1\n",
      "REMOVING REPEATED SENTENCES FROM CLASS 1 THAT ALSO BELONG TO CLASS 0\n",
      "\tRemoved 357 repeated sentences from X1\n",
      "CM [[2367    0]\n",
      " [  54 8073]]\n",
      "F1 0.9887218045112781\n",
      "Acc 0.9948542024013722\n"
     ]
    }
   ],
   "source": [
    "#Load classifier for 'amazon pay' and test it on the entire dataset\n",
    "pkl_filename = 'models/'+columnToUse+\"_new_model.pkl\"\n",
    "with open(pkl_filename, 'rb') as file:\n",
    "    text_clf = pickle.load(file)\n",
    "    \n",
    "#get the data\n",
    "#ignoring None sentences\n",
    "X0 = dfaux.loc[dfaux[columnToUse] == 0]['Sentence'].tolist()\n",
    "xn0 = len(X0)\n",
    "X1 = dfaux.loc[dfaux[columnToUse] == 1]['Sentence'].tolist()\n",
    "xn1 = len(X1)\n",
    "print('ORIGINAL DATASET SAMPLING')\n",
    "print('\\t',len(X0+X1), 'instances to classify, ',xn0, 'for class 0, ', xn1, 'for class 1' )\n",
    "print('REMOVING REPEATED SENTENCES FROM CLASS 1 THAT ALSO BELONG TO CLASS 0')\n",
    "#then check for repeated sentences belonging to both X0 and X1. If it belongs to X0, we remove it from X1.\n",
    "#This happens, sentences are repeated in the dataset as belonging to more than one privacy category, hence we clean it\n",
    "#before letting the classifier learn\n",
    "totalr = 0\n",
    "for s0 in X0:\n",
    "    s0 = s0.strip()\n",
    "    ir = [index for index, s in enumerate(X1) if s.strip()==s0]\n",
    "    ir = sorted(ir, reverse = True) #need to sort in reverse so when deleting an index does not affect the rest of indices\n",
    "    for index in ir:\n",
    "        del X1[index]\n",
    "    totalr += len(ir)\n",
    "print('\\tRemoved', totalr, 'repeated sentences from X1')\n",
    "X = X0+X1\n",
    "xn0 = len(X0);xn1 = len(X1)\n",
    "y = xn0*[0] + xn1*[1]\n",
    "\n",
    "#test the data\n",
    "pred_y = text_clf.predict(X)\n",
    "cm = confusion_matrix(y, pred_y)\n",
    "f1 = f1_score(y, pred_y, average=None)[0]\n",
    "acc = np.mean(pred_y == y)\n",
    "\n",
    "print('CM', cm)\n",
    "print('F1', f1)\n",
    "print('Acc', acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "### Ignoring None sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ORIGINAL DATASET SAMPLING\n",
      "\t 4398 instances to classify,  2367 for class 0,  2031 for class 1\n",
      "REMOVING REPEATED SENTENCES FROM CLASS 1 THAT ALSO BELONG TO CLASS 0\n",
      "\tRemoved 349 repeated sentences from X1\n",
      "CM [[2367    0]\n",
      " [  52 1630]]\n",
      "F1 0.9891349770162976\n",
      "Acc 0.9871573227957521\n"
     ]
    }
   ],
   "source": [
    "#Load classifier for 'amazon pay' and test it on the entire dataset\n",
    "pkl_filename = 'models/'+columnToUse+\"_new_model.pkl\"\n",
    "with open(pkl_filename, 'rb') as file:\n",
    "    text_clf = pickle.load(file)\n",
    "    \n",
    "#get the data\n",
    "#ignoring None sentences\n",
    "X0 = dfaux.loc[(dfaux[columnToUse] == 0)&(dfaux['Permission'] != 'None')]['Sentence'].tolist()\n",
    "xn0 = len(X0)\n",
    "X1 = dfaux.loc[(dfaux[columnToUse] == 1)&(dfaux['Permission'] != 'None')]['Sentence'].tolist()\n",
    "xn1 = len(X1)\n",
    "print('ORIGINAL DATASET SAMPLING')\n",
    "print('\\t',len(X0+X1), 'instances to classify, ',xn0, 'for class 0, ', xn1, 'for class 1' )\n",
    "print('REMOVING REPEATED SENTENCES FROM CLASS 1 THAT ALSO BELONG TO CLASS 0')\n",
    "#then check for repeated sentences belonging to both X0 and X1. If it belongs to X0, we remove it from X1.\n",
    "#This happens, sentences are repeated in the dataset as belonging to more than one privacy category, hence we clean it\n",
    "#before letting the classifier learn\n",
    "totalr = 0\n",
    "for s0 in X0:\n",
    "    s0 = s0.strip()\n",
    "    ir = [index for index, s in enumerate(X1) if s.strip()==s0]\n",
    "    ir = sorted(ir, reverse = True) #need to sort in reverse so when deleting an index does not affect the rest of indices\n",
    "    for index in ir:\n",
    "        del X1[index]\n",
    "    totalr += len(ir)\n",
    "print('\\tRemoved', totalr, 'repeated sentences from X1')\n",
    "X = X0+X1\n",
    "xn0 = len(X0);xn1 = len(X1)\n",
    "y = xn0*[0] + xn1*[1]\n",
    "\n",
    "#test the data\n",
    "pred_y = text_clf.predict(X)\n",
    "cm = confusion_matrix(y, pred_y)\n",
    "f1 = f1_score(y, pred_y, average=None)[0]\n",
    "acc = np.mean(pred_y == y)\n",
    "\n",
    "print('CM', cm)\n",
    "print('F1', f1)\n",
    "print('Acc', acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Classifier for class 'Skill Personisation' - 0.99+"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skill Personisation  -  8  -  Label8\n"
     ]
    }
   ],
   "source": [
    "categoryname = 'Skill Personisation'\n",
    "labelindex = labelToindex[categoryname]\n",
    "columnToUse = 'Label'+str(labelindex)\n",
    "print(categoryname, ' - ', labelindex, ' - ', columnToUse)\n",
    "\n",
    "classproportion = 4\n",
    "ngrams = (1,3)\n",
    "apply_tfidf = False\n",
    "apply_binary = True\n",
    "svm_loss = 'modified_huber'\n",
    "svm_alpha = 0.0001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ORIGINAL DATASET SAMPLING\n",
      "\t 970 sentences of class 0;  10754 sentence for class 1\n",
      "REMOVING REPEATED SENTENCES FROM CLASS 1 THAT ALSO BELONG TO CLASS 0\n",
      "\tRemoved 16 repeated sentences from X1\n",
      "CHECKING THE PROPORTION OF INSTANCES IN EACH CLASS\n",
      "\tclass 1 is larger than 0, applying subsampling\n",
      "\t 970 sentences of class 0;  3880 sentence for class 1\n",
      ">Train: 0=921, 1=3686, Test: 0=49, 1=194\n",
      "CM [[ 49   0]\n",
      " [  0 194]]\n",
      "F1 1.0\n",
      "Acc 1.0\n",
      "*Model for  Label8 saved in  models/Label8_new_model.pkl\n"
     ]
    }
   ],
   "source": [
    "dfaux = df\n",
    "\n",
    "X0 = dfaux.loc[dfaux[columnToUse] == 0]['Sentence'].tolist()\n",
    "X0 = X0 * 10\n",
    "xn0 = len(X0)\n",
    "X1 = dfaux.loc[dfaux[columnToUse] == 1]['Sentence'].tolist()\n",
    "xn1 = len(X1)\n",
    "\n",
    "print('ORIGINAL DATASET SAMPLING')\n",
    "print('\\t',xn0, 'sentences of class 0; ', xn1, 'sentence for class 1')\n",
    "\n",
    "            \n",
    "            \n",
    "print('REMOVING REPEATED SENTENCES FROM CLASS 1 THAT ALSO BELONG TO CLASS 0')\n",
    "#then check for repeated sentences belonging to both X0 and X1. If it belongs to X0, we remove it from X1.\n",
    "#This happens, sentences are repeated in the dataset as belonging to more than one privacy category, hence we clean it\n",
    "#before letting the classifier learn\n",
    "totalr = 0\n",
    "for s0 in X0:\n",
    "    s0 = s0.strip()\n",
    "    ir = [index for index, s in enumerate(X1) if s.strip()==s0]\n",
    "    ir = sorted(ir, reverse = True) #need to sort in reverse so when deleting an index does not affect the rest of indices\n",
    "    for index in ir:\n",
    "        del X1[index]\n",
    "    totalr += len(ir)\n",
    "print('\\tRemoved', totalr, 'repeated sentences from X1')\n",
    "\n",
    "#subsampling (random) of largest class\n",
    "print('CHECKING THE PROPORTION OF INSTANCES IN EACH CLASS')\n",
    "if(len(X1)>len(X0)*classproportion):\n",
    "    X1 = random.sample(X1, xn0*classproportion)\n",
    "    xn1 = len(X1)\n",
    "    print('\\tclass 1 is larger than 0, applying subsampling')\n",
    "elif(len(X0)>len(X1)*classproportion):\n",
    "    X0 = random.sample(X0, xn1*classproportion)\n",
    "    xn0 = len(X0)\n",
    "    print('\\tclass 0 is larger than 1')\n",
    "print('\\t',xn0, 'sentences of class 0; ', xn1, 'sentence for class 1')\n",
    "X = X0+X1\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "text_clf = Pipeline([\n",
    "    ('vect', CountVectorizer(ngram_range=ngrams, binary=apply_binary)),\n",
    "    ('tfidf', TfidfTransformer(use_idf=apply_tfidf)),\n",
    "    ('clf', SGDClassifier(loss=svm_loss, penalty='l2',\n",
    "                          alpha=svm_alpha, max_iter=10000, tol=None,  class_weight='balanced')),\n",
    "    ])\n",
    "\n",
    "labels = xn0*[0] + xn1*[1]\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, labels, test_size=0.05, stratify=labels)\n",
    "\n",
    "#print proportion for training and testing\n",
    "X_train=np.array(X_train);X_test=np.array(X_test);y_train=np.array(y_train);y_test=np.array(y_test);\n",
    "train_0, train_1 = len(y_train[y_train==0]), len(y_train[y_train==1])\n",
    "test_0, test_1 = len(y_test[y_test==0]), len(y_test[y_test==1])\n",
    "print('>Train: 0=%d, 1=%d, Test: 0=%d, 1=%d' % (train_0, train_1, test_0, test_1))\n",
    "\n",
    "\n",
    "#train and test\n",
    "text_clf.fit(X_train, y_train)          \n",
    "pred_y = text_clf.predict(X_test)\n",
    "#plot_confusion_matrix(clf, test_X, test_y)\n",
    "cm = confusion_matrix(y_test, pred_y)\n",
    "f1 = f1_score(y_test, pred_y, average=None)[0]\n",
    "acc = np.mean(pred_y == y_test) \n",
    "\n",
    "print('CM', cm)\n",
    "print('F1', f1)\n",
    "print('Acc', acc)\n",
    "\n",
    "#save model\n",
    "pkl_filename = 'models/'+columnToUse+\"_new_model.pkl\"\n",
    "with open(pkl_filename, 'wb') as file:\n",
    "    pickle.dump(text_clf, file)\n",
    "    print('*Model for ', columnToUse, 'saved in ', pkl_filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "### Without ignoring None sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ORIGINAL DATASET SAMPLING\n",
      "\t 10851 instances to classify,  97 for class 0,  10754 for class 1\n",
      "REMOVING REPEATED SENTENCES FROM CLASS 1 THAT ALSO BELONG TO CLASS 0\n",
      "\tRemoved 16 repeated sentences from X1\n",
      "CM [[   97     0]\n",
      " [    1 10737]]\n",
      "F1 0.9948717948717948\n",
      "Acc 0.9999077065066913\n"
     ]
    }
   ],
   "source": [
    "#Load classifier for 'amazon pay' and test it on the entire dataset\n",
    "pkl_filename = 'models/'+columnToUse+\"_new_model.pkl\"\n",
    "with open(pkl_filename, 'rb') as file:\n",
    "    text_clf = pickle.load(file)\n",
    "    \n",
    "#get the data\n",
    "#ignoring None sentences\n",
    "X0 = dfaux.loc[dfaux[columnToUse] == 0]['Sentence'].tolist()\n",
    "xn0 = len(X0)\n",
    "X1 = dfaux.loc[dfaux[columnToUse] == 1]['Sentence'].tolist()\n",
    "xn1 = len(X1)\n",
    "print('ORIGINAL DATASET SAMPLING')\n",
    "print('\\t',len(X0+X1), 'instances to classify, ',xn0, 'for class 0, ', xn1, 'for class 1' )\n",
    "print('REMOVING REPEATED SENTENCES FROM CLASS 1 THAT ALSO BELONG TO CLASS 0')\n",
    "#then check for repeated sentences belonging to both X0 and X1. If it belongs to X0, we remove it from X1.\n",
    "#This happens, sentences are repeated in the dataset as belonging to more than one privacy category, hence we clean it\n",
    "#before letting the classifier learn\n",
    "totalr = 0\n",
    "for s0 in X0:\n",
    "    s0 = s0.strip()\n",
    "    ir = [index for index, s in enumerate(X1) if s.strip()==s0]\n",
    "    ir = sorted(ir, reverse = True) #need to sort in reverse so when deleting an index does not affect the rest of indices\n",
    "    for index in ir:\n",
    "        del X1[index]\n",
    "    totalr += len(ir)\n",
    "print('\\tRemoved', totalr, 'repeated sentences from X1')\n",
    "X = X0+X1\n",
    "xn0 = len(X0);xn1 = len(X1)\n",
    "y = xn0*[0] + xn1*[1]\n",
    "\n",
    "#test the data\n",
    "pred_y = text_clf.predict(X)\n",
    "cm = confusion_matrix(y, pred_y)\n",
    "f1 = f1_score(y, pred_y, average=None)[0]\n",
    "acc = np.mean(pred_y == y)\n",
    "\n",
    "print('CM', cm)\n",
    "print('F1', f1)\n",
    "print('Acc', acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "### Ignoring None sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ORIGINAL DATASET SAMPLING\n",
      "\t 4398 instances to classify,  97 for class 0,  4301 for class 1\n",
      "REMOVING REPEATED SENTENCES FROM CLASS 1 THAT ALSO BELONG TO CLASS 0\n",
      "\tRemoved 16 repeated sentences from X1\n",
      "CM [[  97    0]\n",
      " [   0 4285]]\n",
      "F1 1.0\n",
      "Acc 1.0\n"
     ]
    }
   ],
   "source": [
    "#Load classifier for 'amazon pay' and test it on the entire dataset\n",
    "pkl_filename = 'models/'+columnToUse+\"_new_model.pkl\"\n",
    "with open(pkl_filename, 'rb') as file:\n",
    "    text_clf = pickle.load(file)\n",
    "    \n",
    "#get the data\n",
    "#ignoring None sentences\n",
    "X0 = dfaux.loc[(dfaux[columnToUse] == 0)&(dfaux['Permission'] != 'None')]['Sentence'].tolist()\n",
    "xn0 = len(X0)\n",
    "X1 = dfaux.loc[(dfaux[columnToUse] == 1)&(dfaux['Permission'] != 'None')]['Sentence'].tolist()\n",
    "xn1 = len(X1)\n",
    "print('ORIGINAL DATASET SAMPLING')\n",
    "print('\\t',len(X0+X1), 'instances to classify, ',xn0, 'for class 0, ', xn1, 'for class 1' )\n",
    "print('REMOVING REPEATED SENTENCES FROM CLASS 1 THAT ALSO BELONG TO CLASS 0')\n",
    "#then check for repeated sentences belonging to both X0 and X1. If it belongs to X0, we remove it from X1.\n",
    "#This happens, sentences are repeated in the dataset as belonging to more than one privacy category, hence we clean it\n",
    "#before letting the classifier learn\n",
    "totalr = 0\n",
    "for s0 in X0:\n",
    "    s0 = s0.strip()\n",
    "    ir = [index for index, s in enumerate(X1) if s.strip()==s0]\n",
    "    ir = sorted(ir, reverse = True) #need to sort in reverse so when deleting an index does not affect the rest of indices\n",
    "    for index in ir:\n",
    "        del X1[index]\n",
    "    totalr += len(ir)\n",
    "print('\\tRemoved', totalr, 'repeated sentences from X1')\n",
    "X = X0+X1\n",
    "xn0 = len(X0);xn1 = len(X1)\n",
    "y = xn0*[0] + xn1*[1]\n",
    "\n",
    "#test the data\n",
    "pred_y = text_clf.predict(X)\n",
    "cm = confusion_matrix(y, pred_y)\n",
    "f1 = f1_score(y, pred_y, average=None)[0]\n",
    "acc = np.mean(pred_y == y)\n",
    "\n",
    "print('CM', cm)\n",
    "print('F1', f1)\n",
    "print('Acc', acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Classifier for class 'None' - 0.99 \n",
    "training with 80% of class 1, and 3k instnces of class 0 (which has almost 30k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None  -  9  -  Label9\n"
     ]
    }
   ],
   "source": [
    "categoryname = 'None'\n",
    "labelindex = labelToindex[categoryname]\n",
    "columnToUse = 'Label'+str(labelindex)\n",
    "print(categoryname, ' - ', labelindex, ' - ', columnToUse)\n",
    "\n",
    "classproportion = 1\n",
    "ngrams = (1,3)\n",
    "apply_tfidf = False\n",
    "apply_binary = True\n",
    "svm_loss = 'modified_huber'\n",
    "svm_alpha = 0.0001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ORIGINAL DATASET SAMPLING\n",
      "\t 64650 sentences of class 0;  4082 sentence for class 1\n",
      "REMOVING REPEATED SENTENCES FROM CLASS 1 THAT ALSO BELONG TO CLASS 0\n",
      "\tRemoved 13 repeated sentences from X1\n",
      "CHECKING THE PROPORTION OF INSTANCES IN EACH CLASS\n",
      "\tclass 0 is larger than 1\n",
      "\t 4082 sentences of class 0;  4082 sentence for class 1\n",
      ">Train: 0=3265, 1=3255, Test: 0=817, 1=814\n",
      "CM [[815   2]\n",
      " [ 11 803]]\n",
      "F1 0.9920876445526476\n",
      "Acc 0.9920294297976702\n",
      "*Model for  Label9 saved in  models/Label9_new_model.pkl\n"
     ]
    }
   ],
   "source": [
    "dfaux = df\n",
    "\n",
    "X0 = dfaux.loc[dfaux[columnToUse] == 0]['Sentence'].tolist()\n",
    "X0 = X0 * 10\n",
    "xn0 = len(X0)\n",
    "X1 = dfaux.loc[dfaux[columnToUse] == 1]['Sentence'].tolist()\n",
    "xn1 = len(X1)\n",
    "\n",
    "print('ORIGINAL DATASET SAMPLING')\n",
    "print('\\t',xn0, 'sentences of class 0; ', xn1, 'sentence for class 1')\n",
    "\n",
    "#print('INCREASING DATASET WITH APP350')\n",
    "#relevantfiles = ['data/App350/Contact_Phone_Number_1stParty.txt',\n",
    "#                 'data/App350/Contact_Phone_Number_3rdParty.txt']\n",
    "#for filepath in relevantfiles:\n",
    "#    with open(filepath, encoding='utf8') as f:   #utf8      \n",
    "#        text = f.readlines()\n",
    "#        text = [k.replace('\\n', '').strip() for k in text]\n",
    "#        print('\\t', len(text), 'new sentences found in ', filepath)\n",
    "#        for t in text:\n",
    "#            dfaux = dfaux.append({'Permission':categoryname, 'Sentence':t, 'Label':labelindex, columnToUse:0},  ignore_index=True)\n",
    "#            \n",
    "#X0 = dfaux.loc[dfaux[columnToUse] == 0]['Sentence'].tolist()\n",
    "#X0 = X0*20\n",
    "#xn0 = len(X0)\n",
    "#X1 = dfaux.loc[dfaux[columnToUse] == 1]['Sentence'].tolist()\n",
    "#xn1 = len(X1)\n",
    "#print('\\t',xn0, 'sentences of class 0; ', xn1, 'sentence for class 1')\n",
    "            \n",
    "            \n",
    "print('REMOVING REPEATED SENTENCES FROM CLASS 1 THAT ALSO BELONG TO CLASS 0')\n",
    "#then check for repeated sentences belonging to both X0 and X1. If it belongs to X0, we remove it from X1.\n",
    "#This happens, sentences are repeated in the dataset as belonging to more than one privacy category, hence we clean it\n",
    "#before letting the classifier learn\n",
    "totalr = 0\n",
    "for s0 in X0:\n",
    "    s0 = s0.strip()\n",
    "    ir = [index for index, s in enumerate(X1) if s.strip()==s0]\n",
    "    ir = sorted(ir, reverse = True) #need to sort in reverse so when deleting an index does not affect the rest of indices\n",
    "    for index in ir:\n",
    "        del X1[index]\n",
    "    totalr += len(ir)\n",
    "print('\\tRemoved', totalr, 'repeated sentences from X1')\n",
    "\n",
    "#subsampling (random) of largest class\n",
    "print('CHECKING THE PROPORTION OF INSTANCES IN EACH CLASS')\n",
    "if(len(X1)>len(X0)*classproportion):\n",
    "    X1 = random.sample(X1, xn0*classproportion)\n",
    "    xn1 = len(X1)\n",
    "    print('\\tclass 1 is larger than 0, applying subsampling')\n",
    "elif(len(X0)>len(X1)*classproportion):\n",
    "    X0 = random.sample(X0, xn1*classproportion)\n",
    "    xn0 = len(X0)\n",
    "    print('\\tclass 0 is larger than 1')\n",
    "print('\\t',xn0, 'sentences of class 0; ', xn1, 'sentence for class 1')\n",
    "X = X0+X1\n",
    "xn0 = len(X0)\n",
    "xn1 = len(X1)\n",
    "\n",
    "\n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "text_clf = Pipeline([\n",
    "    ('vect', CountVectorizer(ngram_range=ngrams, binary=apply_binary)),\n",
    "    ('tfidf', TfidfTransformer(use_idf=apply_tfidf)),\n",
    "    ('clf', SGDClassifier(loss=svm_loss, penalty='l2',\n",
    "                          alpha=svm_alpha, max_iter=10000, tol=None,  class_weight='balanced')),\n",
    "    ])\n",
    "\n",
    "labels = xn0*[0] + xn1*[1]\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, labels, test_size=0.20, stratify=labels)\n",
    "\n",
    "#print proportion for training and testing\n",
    "X_train=np.array(X_train);X_test=np.array(X_test);y_train=np.array(y_train);y_test=np.array(y_test);\n",
    "train_0, train_1 = len(y_train[y_train==0]), len(y_train[y_train==1])\n",
    "test_0, test_1 = len(y_test[y_test==0]), len(y_test[y_test==1])\n",
    "print('>Train: 0=%d, 1=%d, Test: 0=%d, 1=%d' % (train_0, train_1, test_0, test_1))\n",
    "\n",
    "\n",
    "#train and test\n",
    "text_clf.fit(X_train, y_train)          \n",
    "pred_y = text_clf.predict(X_test)\n",
    "#plot_confusion_matrix(clf, test_X, test_y)\n",
    "cm = confusion_matrix(y_test, pred_y)\n",
    "f1 = f1_score(y_test, pred_y, average=None)[0]\n",
    "acc = np.mean(pred_y == y_test) \n",
    "\n",
    "print('CM', cm)\n",
    "print('F1', f1)\n",
    "print('Acc', acc)\n",
    "\n",
    "#save model\n",
    "pkl_filename = 'models/'+columnToUse+\"_new_model.pkl\"\n",
    "with open(pkl_filename, 'wb') as file:\n",
    "    pickle.dump(text_clf, file)\n",
    "    print('*Model for ', columnToUse, 'saved in ', pkl_filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "### Without ignoring None sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ORIGINAL DATASET SAMPLING\n",
      "\t 10547 instances to classify,  6465 for class 0,  4082 for class 1\n",
      "REMOVING REPEATED SENTENCES FROM CLASS 1 THAT ALSO BELONG TO CLASS 0\n",
      "\tRemoved 13 repeated sentences from X1\n",
      "CM [[6426   39]\n",
      " [  11 4058]]\n",
      "F1 0.9961246318400248\n",
      "Acc 0.9952534649705714\n"
     ]
    }
   ],
   "source": [
    "#Load classifier for 'amazon pay' and test it on the entire dataset\n",
    "pkl_filename = 'models/'+columnToUse+\"_new_model.pkl\"\n",
    "with open(pkl_filename, 'rb') as file:\n",
    "    text_clf = pickle.load(file)\n",
    "    \n",
    "#get the data\n",
    "#ignoring None sentences\n",
    "X0 = dfaux.loc[dfaux[columnToUse] == 0]['Sentence'].tolist()\n",
    "xn0 = len(X0)\n",
    "X1 = dfaux.loc[dfaux[columnToUse] == 1]['Sentence'].tolist()\n",
    "xn1 = len(X1)\n",
    "print('ORIGINAL DATASET SAMPLING')\n",
    "print('\\t',len(X0+X1), 'instances to classify, ',xn0, 'for class 0, ', xn1, 'for class 1' )\n",
    "print('REMOVING REPEATED SENTENCES FROM CLASS 1 THAT ALSO BELONG TO CLASS 0')\n",
    "#then check for repeated sentences belonging to both X0 and X1. If it belongs to X0, we remove it from X1.\n",
    "#This happens, sentences are repeated in the dataset as belonging to more than one privacy category, hence we clean it\n",
    "#before letting the classifier learn\n",
    "totalr = 0\n",
    "for s0 in X0:\n",
    "    s0 = s0.strip()\n",
    "    ir = [index for index, s in enumerate(X1) if s.strip()==s0]\n",
    "    ir = sorted(ir, reverse = True) #need to sort in reverse so when deleting an index does not affect the rest of indices\n",
    "    for index in ir:\n",
    "        del X1[index]\n",
    "    totalr += len(ir)\n",
    "print('\\tRemoved', totalr, 'repeated sentences from X1')\n",
    "X = X0+X1\n",
    "xn0 = len(X0);xn1 = len(X1)\n",
    "y = xn0*[0] + xn1*[1]\n",
    "\n",
    "#test the data\n",
    "pred_y = text_clf.predict(X)\n",
    "cm = confusion_matrix(y, pred_y)\n",
    "f1 = f1_score(y, pred_y, average=None)[0]\n",
    "acc = np.mean(pred_y == y)\n",
    "\n",
    "print('CM', cm)\n",
    "print('F1', f1)\n",
    "print('Acc', acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Quick classifier test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "X = [\n",
    "    'zip code',\n",
    "    'zip',\n",
    "    'zip address',\n",
    "    'email',\n",
    "    'other',\n",
    "    'name'\n",
    "]\n",
    "\n",
    "Y = [1,1,1,0,0,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">Train: 0=2, 1=2, Test: 0=1, 1=1\n",
      "CM [[1 0]\n",
      " [0 1]]\n",
      "F1 1.0\n",
      "Acc 1.0\n"
     ]
    }
   ],
   "source": [
    "text_clf = Pipeline([\n",
    "    ('vect', CountVectorizer(ngram_range=ngrams, binary=apply_binary)),\n",
    "    ('tfidf', TfidfTransformer(use_idf=apply_tfidf)),\n",
    "    ('clf', SGDClassifier(loss=svm_loss, penalty='l2',\n",
    "                          alpha=svm_alpha, max_iter=10000, tol=None,  class_weight='balanced')),\n",
    "    ])\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=0.33, stratify=Y)\n",
    "\n",
    "#print proportion for training and testing\n",
    "X_train=np.array(X_train);X_test=np.array(X_test);y_train=np.array(y_train);y_test=np.array(y_test);\n",
    "train_0, train_1 = len(y_train[y_train==0]), len(y_train[y_train==1])\n",
    "test_0, test_1 = len(y_test[y_test==0]), len(y_test[y_test==1])\n",
    "print('>Train: 0=%d, 1=%d, Test: 0=%d, 1=%d' % (train_0, train_1, test_0, test_1))\n",
    "\n",
    "\n",
    "#train and test\n",
    "text_clf.fit(X_train, y_train)          \n",
    "pred_y = text_clf.predict(X_test)\n",
    "#plot_confusion_matrix(clf, test_X, test_y)\n",
    "cm = confusion_matrix(y_test, pred_y)\n",
    "f1 = f1_score(y_test, pred_y, average=None)[0]\n",
    "acc = np.mean(pred_y == y_test) \n",
    "\n",
    "print('CM', cm)\n",
    "print('F1', f1)\n",
    "print('Acc', acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1 1 1 0 0]\n"
     ]
    }
   ],
   "source": [
    "test_sentences = ['we collect zip addres', 'postal address', 'zip code', 'email', 'test']\n",
    "pred_y = text_clf.predict(test_sentences)\n",
    "print(pred_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
