{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AnalysePhoneticsBetweenSkills"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "In this notebook I perform some analysis between the phonetics of the different skills, using the CMU phonetic dictionary, and the levehnstein distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#laod CMU dict\n",
    "CMUpath = 'cmudict_SPHINX_40.txt'\n",
    "cmu_dict = {}\n",
    "with open(CMUpath, \"r\", encoding='utf8') as f:\n",
    "    for line in f:\n",
    "        #A line looks like this\n",
    "        #ABORT\tAH B AO R T\n",
    "        if line and len(line.strip())>0:\n",
    "            spl = line.split('\\t')\n",
    "            word = spl[0].strip()\n",
    "            phonemes = spl[1].strip()\n",
    "            cmu_dict[word] = phonemes.split(' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* 123feeli_ng down!\n",
      "['W', 'AH', 'N', ' ', 'T', 'UW', ' ', 'TH', 'R', 'IY', ' ', 'F', 'IY', 'L', 'IH', 'NG', ' ', 'D', 'AW', 'N']\n",
      "* Apple watch --!\n",
      "['AE', 'P', 'AH', 'L', ' ', 'W', 'AA', 'CH']\n",
      "* Unexxxxisting word but the rest is fine\n",
      "['_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', ' ', 'W', 'ER', 'D', ' ', 'B', 'AH', 'T', ' ', 'DH', 'AH', ' ', 'R', 'EH', 'S', 'T', ' ', 'IH', 'Z', ' ', 'F', 'AY', 'N']\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "Call getPhoneticTranslation to get the phonetic trasnlation of a sentence. \n",
    "It cleans the word and obtains a CMU translation of the sentence.\n",
    "Replaces the non existent words in the dictionary by a word of the same length formed by '_'\n",
    "'''\n",
    "\n",
    "import string\n",
    "invalidChars = set(string.punctuation)\n",
    "numbers = {'0':'ZERO','1':'ONE','2':'TWO','3':'THREE','4':'FOUR','5':'FIVE',\n",
    "           '6':'SIX','7':'SEVEN','8':'EIGHT','9':'NINE'}\n",
    "def getPhoneticTranslation(sentence):\n",
    "    #Transcriptions to be compared were converted to uppercase and all non-ASCII characters and punctuation were\n",
    "    #removed. Digits were replaced with their corresponding texts (e.g., “2” became “two”)\n",
    "    def PreProcessWord(word):\n",
    "        for c in invalidChars:\n",
    "            if c in word:\n",
    "                word = word.replace(c,'')\n",
    "        for n in numbers.keys():\n",
    "            if n in word:\n",
    "                word = word.replace(n, ' '+numbers[n]+' ')\n",
    "\n",
    "        word = word.upper()\n",
    "        word = word.replace('  ', ' ')\n",
    "        return word.strip()\n",
    "\n",
    "    a=PreProcessWord(sentence)\n",
    "    cmutranslated = []\n",
    "    for w in a.split(' '):\n",
    "        if w in cmu_dict:\n",
    "            b = cmu_dict[w]\n",
    "        else:\n",
    "            b = ['_']*len(w)\n",
    "        cmutranslated+= b+[' ']\n",
    "    return cmutranslated[:len(cmutranslated)-1]\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "#example of the process\n",
    "word = '123feeli_ng down!'\n",
    "print('*', word)\n",
    "print(getPhoneticTranslation(word))\n",
    "word = 'Apple watch --!'\n",
    "print('*', word)\n",
    "print(getPhoneticTranslation(word))\n",
    "word = 'Unexxxxisting word but the rest is fine'\n",
    "print('*', word)\n",
    "print(getPhoneticTranslation(word))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load new Alexa Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load dataset\n",
    "import os\n",
    "import json\n",
    "\n",
    "def reloadData():\n",
    "    def getFiles(dirName):\n",
    "        listOfFile = os.listdir(dirName)\n",
    "        completeFileList = list()\n",
    "        for file in listOfFile:\n",
    "            completePath = os.path.join(dirName, file)\n",
    "            if os.path.isdir(completePath):\n",
    "                completeFileList = completeFileList + getFiles(completePath)\n",
    "            elif '.json' in completePath:\n",
    "                completeFileList.append(completePath.replace('\\\\', '/'))\n",
    "\n",
    "        return completeFileList\n",
    "\n",
    "    allf = getFiles( \"../NewFullDataset/\")\n",
    "    print(allf)\n",
    "\n",
    "    #create dictionary of markets and skills\n",
    "    skillsmarketd = {}\n",
    "    for f in allf:\n",
    "        marketname = f.split('/')[2].split('.json')[0]\n",
    "        with open(f, 'r') as json_file:\n",
    "            skillsmarketd[marketname] = json.load(json_file)\n",
    "\n",
    "    print()\n",
    "    print('Total unique skills per market as per last week')\n",
    "    for key,value in skillsmarketd.items():\n",
    "        print(key, \": \", len(value))\n",
    "    print()\n",
    "    print()\n",
    "    return skillsmarketd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['../NewFullDataset/AU.json', '../NewFullDataset/CA.json', '../NewFullDataset/DE.json', '../NewFullDataset/ES.json', '../NewFullDataset/FR.json', '../NewFullDataset/IN.json', '../NewFullDataset/IT.json', '../NewFullDataset/JP.json', '../NewFullDataset/MX.json', '../NewFullDataset/UK.json', '../NewFullDataset/US.json']\n",
      "\n",
      "Total unique skills per market as per last week\n",
      "AU :  24062\n",
      "CA :  26009\n",
      "DE :  9082\n",
      "ES :  4756\n",
      "FR :  2282\n",
      "IN :  29249\n",
      "IT :  4202\n",
      "JP :  3519\n",
      "MX :  1972\n",
      "UK :  30975\n",
      "US :  60298\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "allskills = reloadData()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Levenshtein Distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import editdistance\n",
    "#luckily I found this iplementation of levehenstein distance that goes almost 100 times faster than the defined above...\n",
    "def levdistance(w1,w2):\n",
    "    #if there is a difference of more than 3 words in the skil names, \n",
    "    #we are not interested in the real distance as they are not interesting\n",
    "    if(w1 == w2):\n",
    "        return 0\n",
    "    if(len(w1) > len(w2)+2):\n",
    "        return 1\n",
    "    if(len(w2) > len(w1)+2):\n",
    "        return 1\n",
    "    return editdistance.eval(w1, w2)/ max(len(w1), len(w2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.3333333333333333"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "levdistance(['ck', 'a', 't'], ['k', 'a', 't'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Perform experiment for TABLE III.SKILL NAMES WITHLEVENSHTEIN DISTANCE≤0.2, considering english markets and ignoring words that do not have a CMU translation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### NOTE: Do not run this, it takes a while!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['../NewFullDataset/AU.json', '../NewFullDataset/CA.json', '../NewFullDataset/DE.json', '../NewFullDataset/ES.json', '../NewFullDataset/FR.json', '../NewFullDataset/IN.json', '../NewFullDataset/IT.json', '../NewFullDataset/JP.json', '../NewFullDataset/MX.json', '../NewFullDataset/UK.json', '../NewFullDataset/US.json']\n",
      "\n",
      "Total unique skills per market as per last week\n",
      "AU :  24062\n",
      "CA :  26009\n",
      "DE :  9082\n",
      "ES :  4756\n",
      "FR :  2282\n",
      "IN :  29249\n",
      "IT :  4202\n",
      "JP :  3519\n",
      "MX :  1972\n",
      "UK :  30975\n",
      "US :  60298\n",
      "\n",
      "\n",
      "Exploring market UK\n",
      "finished creating crate skilltocmu dict for market UK\n",
      "\t[ 9 : 21 . 37 ] Distances computed  0\n",
      "\t[ 9 : 22 . 5 ] Distances computed  1000\n",
      "\t[ 9 : 22 . 30 ] Distances computed  2000\n",
      "\t[ 9 : 22 . 55 ] Distances computed  3000\n",
      "\t[ 9 : 23 . 19 ] Distances computed  4000\n",
      "\t[ 9 : 23 . 40 ] Distances computed  5000\n",
      "\t[ 9 : 24 . 0 ] Distances computed  6000\n",
      "\t[ 9 : 24 . 20 ] Distances computed  7000\n",
      "\t[ 9 : 24 . 39 ] Distances computed  8000\n",
      "\t[ 9 : 24 . 54 ] Distances computed  9000\n",
      "\t[ 9 : 25 . 9 ] Distances computed  10000\n",
      "\t[ 9 : 25 . 22 ] Distances computed  11000\n",
      "\t[ 9 : 25 . 34 ] Distances computed  12000\n",
      "\t[ 9 : 25 . 44 ] Distances computed  13000\n",
      "\t[ 9 : 25 . 52 ] Distances computed  14000\n",
      "\t[ 9 : 25 . 59 ] Distances computed  15000\n",
      "\t[ 9 : 26 . 4 ] Distances computed  16000\n",
      "\t[ 9 : 26 . 9 ] Distances computed  17000\n",
      "\t[ 9 : 26 . 11 ] Distances computed  18000\n",
      "Saving json in  PhonDict/UK.json\n",
      "[ 9 : 26 . 12 ] Finished estiamting skill names levehenstein distances for market UK\n",
      "Exploring market AU\n",
      "finished creating crate skilltocmu dict for market AU\n",
      "\t[ 9 : 26 . 12 ] Distances computed  0\n",
      "\t[ 9 : 26 . 36 ] Distances computed  1000\n",
      "\t[ 9 : 26 . 59 ] Distances computed  2000\n",
      "\t[ 9 : 27 . 20 ] Distances computed  3000\n",
      "\t[ 9 : 27 . 39 ] Distances computed  4000\n",
      "\t[ 9 : 27 . 56 ] Distances computed  5000\n",
      "\t[ 9 : 28 . 13 ] Distances computed  6000\n",
      "\t[ 9 : 28 . 27 ] Distances computed  7000\n",
      "\t[ 9 : 28 . 40 ] Distances computed  8000\n",
      "\t[ 9 : 28 . 51 ] Distances computed  9000\n",
      "\t[ 9 : 29 . 0 ] Distances computed  10000\n",
      "\t[ 9 : 29 . 8 ] Distances computed  11000\n",
      "\t[ 9 : 29 . 14 ] Distances computed  12000\n",
      "\t[ 9 : 29 . 18 ] Distances computed  13000\n",
      "\t[ 9 : 29 . 21 ] Distances computed  14000\n",
      "\t[ 9 : 29 . 22 ] Distances computed  15000\n",
      "Saving json in  PhonDict/AU.json\n",
      "[ 9 : 29 . 23 ] Finished estiamting skill names levehenstein distances for market AU\n",
      "Exploring market CA\n",
      "finished creating crate skilltocmu dict for market CA\n",
      "\t[ 9 : 29 . 24 ] Distances computed  0\n",
      "\t[ 9 : 29 . 48 ] Distances computed  1000\n",
      "\t[ 9 : 30 . 10 ] Distances computed  2000\n",
      "\t[ 9 : 30 . 33 ] Distances computed  3000\n",
      "\t[ 9 : 30 . 54 ] Distances computed  4000\n",
      "\t[ 9 : 31 . 12 ] Distances computed  5000\n",
      "\t[ 9 : 31 . 30 ] Distances computed  6000\n",
      "\t[ 9 : 31 . 44 ] Distances computed  7000\n",
      "\t[ 9 : 31 . 57 ] Distances computed  8000\n",
      "\t[ 9 : 32 . 7 ] Distances computed  9000\n",
      "\t[ 9 : 32 . 17 ] Distances computed  10000\n",
      "\t[ 9 : 32 . 25 ] Distances computed  11000\n",
      "\t[ 9 : 32 . 32 ] Distances computed  12000\n",
      "\t[ 9 : 32 . 38 ] Distances computed  13000\n",
      "\t[ 9 : 32 . 42 ] Distances computed  14000\n",
      "\t[ 9 : 32 . 45 ] Distances computed  15000\n",
      "\t[ 9 : 32 . 47 ] Distances computed  16000\n",
      "Saving json in  PhonDict/CA.json\n",
      "[ 9 : 32 . 48 ] Finished estiamting skill names levehenstein distances for market CA\n",
      "Exploring market IN\n",
      "finished creating crate skilltocmu dict for market IN\n",
      "\t[ 9 : 32 . 48 ] Distances computed  0\n",
      "\t[ 9 : 33 . 14 ] Distances computed  1000\n",
      "\t[ 9 : 33 . 39 ] Distances computed  2000\n",
      "\t[ 9 : 34 . 2 ] Distances computed  3000\n",
      "\t[ 9 : 34 . 22 ] Distances computed  4000\n",
      "\t[ 9 : 34 . 40 ] Distances computed  5000\n",
      "\t[ 9 : 34 . 58 ] Distances computed  6000\n",
      "\t[ 9 : 35 . 14 ] Distances computed  7000\n",
      "\t[ 9 : 35 . 29 ] Distances computed  8000\n",
      "\t[ 9 : 35 . 41 ] Distances computed  9000\n",
      "\t[ 9 : 35 . 52 ] Distances computed  10000\n",
      "\t[ 9 : 36 . 2 ] Distances computed  11000\n",
      "\t[ 9 : 36 . 9 ] Distances computed  12000\n",
      "\t[ 9 : 36 . 15 ] Distances computed  13000\n",
      "\t[ 9 : 36 . 20 ] Distances computed  14000\n",
      "\t[ 9 : 36 . 24 ] Distances computed  15000\n",
      "\t[ 9 : 36 . 25 ] Distances computed  16000\n",
      "Saving json in  PhonDict/IN.json\n",
      "[ 9 : 36 . 26 ] Finished estiamting skill names levehenstein distances for market IN\n",
      "Exploring market US\n",
      "finished creating crate skilltocmu dict for market US\n",
      "\t[ 9 : 36 . 27 ] Distances computed  0\n",
      "\t[ 9 : 37 . 18 ] Distances computed  1000\n",
      "\t[ 9 : 38 . 8 ] Distances computed  2000\n",
      "\t[ 9 : 39 . 1 ] Distances computed  3000\n",
      "\t[ 9 : 39 . 55 ] Distances computed  4000\n",
      "\t[ 9 : 40 . 46 ] Distances computed  5000\n",
      "\t[ 9 : 41 . 33 ] Distances computed  6000\n",
      "\t[ 9 : 42 . 20 ] Distances computed  7000\n",
      "\t[ 9 : 43 . 8 ] Distances computed  8000\n",
      "\t[ 9 : 43 . 54 ] Distances computed  9000\n",
      "\t[ 9 : 44 . 38 ] Distances computed  10000\n",
      "\t[ 9 : 45 . 20 ] Distances computed  11000\n",
      "\t[ 9 : 45 . 59 ] Distances computed  12000\n",
      "\t[ 9 : 46 . 39 ] Distances computed  13000\n",
      "\t[ 9 : 47 . 7 ] Distances computed  14000\n",
      "\t[ 9 : 47 . 36 ] Distances computed  15000\n",
      "\t[ 9 : 48 . 8 ] Distances computed  16000\n",
      "\t[ 9 : 48 . 41 ] Distances computed  17000\n",
      "\t[ 9 : 49 . 12 ] Distances computed  18000\n",
      "\t[ 9 : 49 . 42 ] Distances computed  19000\n",
      "\t[ 9 : 50 . 11 ] Distances computed  20000\n",
      "\t[ 9 : 50 . 37 ] Distances computed  21000\n",
      "\t[ 9 : 51 . 2 ] Distances computed  22000\n",
      "\t[ 9 : 51 . 26 ] Distances computed  23000\n",
      "\t[ 9 : 51 . 47 ] Distances computed  24000\n",
      "\t[ 9 : 52 . 6 ] Distances computed  25000\n",
      "\t[ 9 : 52 . 24 ] Distances computed  26000\n",
      "\t[ 9 : 52 . 39 ] Distances computed  27000\n",
      "\t[ 9 : 52 . 55 ] Distances computed  28000\n",
      "\t[ 9 : 53 . 9 ] Distances computed  29000\n",
      "\t[ 9 : 53 . 22 ] Distances computed  30000\n",
      "\t[ 9 : 53 . 34 ] Distances computed  31000\n",
      "\t[ 9 : 53 . 44 ] Distances computed  32000\n",
      "\t[ 9 : 53 . 51 ] Distances computed  33000\n",
      "\t[ 9 : 53 . 58 ] Distances computed  34000\n",
      "\t[ 9 : 54 . 2 ] Distances computed  35000\n",
      "\t[ 9 : 54 . 5 ] Distances computed  36000\n",
      "\t[ 9 : 54 . 7 ] Distances computed  37000\n",
      "Saving json in  PhonDict/US.json\n",
      "[ 9 : 54 . 9 ] Finished estiamting skill names levehenstein distances for market US\n"
     ]
    }
   ],
   "source": [
    "import datetime\n",
    "\n",
    "#explore the different markets per separate, otherwis eI get memory errors\n",
    "data = reloadData()\n",
    "englishmarkets = ['UK','AU', 'CA', 'IN', 'US']\n",
    "for market in englishmarkets:\n",
    "    uniqskcount = {} #dictionary that contains the quantity of skills for each element\n",
    "    skilltocmu = {}  #set of unique skill names considered and their translation to phonetics\n",
    "    \n",
    "    '''\n",
    "    Create skilltocmu dict\n",
    "    '''\n",
    "    print('Exploring market', market)\n",
    "    skills = data[market]\n",
    "    for skillid, skillobj in skills.items():\n",
    "        name1 = skillobj['name'].lower()\n",
    "        cmuname = getPhoneticTranslation(name1)\n",
    "        #we ignore skills we couldnt completely trasnalte (include '_') [also to speed up the process]\n",
    "        if '_' not in cmuname:\n",
    "            skilltocmu[name1] = cmuname\n",
    "            if(name1 in uniqskcount):\n",
    "                uniqskcount[name1] += 1\n",
    "            else:\n",
    "                uniqskcount[name1] = 1\n",
    "    print('finished creating crate skilltocmu dict for market', market)    \n",
    "\n",
    "    '''\n",
    "    Calculate levehenstein distances:\n",
    "    This process is super slow. We do many tricks to speed up the process:\n",
    "        1- since we are only interested in super similar skill names, we ignore words with different lengths (see lev function above)\n",
    "        2- we only iterate trhough unique skill names transformed to lower, hence the results may be different as \n",
    "        in other figures since 'catFacts' and 'CatFacts' are considered the same skill\n",
    "        3- We only identify which skill names have levehenstein distances < 0.1 or 0.2 (to reduce the final json files).\n",
    "        Then, we count how many of skills with this name.lower() exist in the market to get hte data for table III\n",
    "    '''\n",
    "    alldistances = set([])\n",
    "    uniqsk = list(skilltocmu.keys())\n",
    "    totalanalysed = 0\n",
    "    for i in range(len(uniqsk)):\n",
    "        name1 = uniqsk[i].lower()\n",
    "        if(name1 not in skilltocmu):\n",
    "            continue\n",
    "        cmuname1 = skilltocmu[name1]\n",
    "        for j in range(i, len(uniqsk)):\n",
    "            name2 = uniqsk[j].lower()\n",
    "            if(name2 not in skilltocmu):\n",
    "                continue\n",
    "            cmuname2 = skilltocmu[name2]\n",
    "            d=  levdistance(cmuname1, cmuname2)\n",
    "            if(d<0.31):\n",
    "                #only save these skills with smaller lev distances (otherwise memory error)\n",
    "                alldistances.add( (d,name1,name2) )\n",
    "            #count all distances processed\n",
    "            totalanalysed +=1\n",
    "            \n",
    "        if(i%1000 == 0):\n",
    "            now = datetime.datetime.now()\n",
    "            print('\\t[',now.hour,':',now.minute,'.',now.second,'] Distances computed ', i)\n",
    "     \n",
    "    \n",
    "    \n",
    "    #save analysis in PhonDict json\n",
    "    path = 'PhonDict/'+market+'.json'\n",
    "    with open(path, 'w') as json_file:\n",
    "        print('Saving json in ',path)\n",
    "        json.dump( {'phonetic_distances_minororequal03':list(alldistances), 'all_distances_processed':totalanalysed, 'skillnames_count_d':uniqskcount} , json_file)    \n",
    "    \n",
    "    #min01 = [d for d in alldistances if d <= 0.1 and d!=0]\n",
    "    #min02 = [d for d in alldistances if d <= 0.2 and d!=0]\n",
    "    ##we should count how many times this skills appear in the market!\n",
    "    #print('There are ', len(min01), 'skills within a ld of 0.1')\n",
    "    #print('There are ', len(min02), 'skills within a ld of 0.2')\n",
    "    #print('Total unique skill names ', len(uniqsk))\n",
    "    #\n",
    "    now = datetime.datetime.now()\n",
    "    print('[',now.hour,':',now.minute,'.',now.second,'] Finished estiamting skill names levehenstein distances for market', market)    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the dictionaries of words and get the data for table III"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded json for  AU  : PhonDict/AU.json\n",
      "Loaded json for  CA  : PhonDict/CA.json\n",
      "Loaded json for  IN  : PhonDict/IN.json\n",
      "Loaded json for  UK  : PhonDict/UK.json\n",
      "Loaded json for  US  : PhonDict/US.json\n",
      "**Studying market AU\n",
      "Total hits for unique skill names.lower()  17974\n",
      "There are  673 skills within a ld of 0.1 to any other skillname that does not contain \"_\", addding a total of hits  1234 ( 0.06865472348948481 %)\n",
      "There are  3099 skills within a ld of 0.1 to any other skillname that does not contain \"_\", addding a total of hits  4439 ( 0.24696784243907868 %)\n",
      "**Studying market CA\n",
      "Total hits for unique skill names.lower()  19209\n",
      "There are  912 skills within a ld of 0.1 to any other skillname that does not contain \"_\", addding a total of hits  1514 ( 0.07881722109427872 %)\n",
      "There are  3414 skills within a ld of 0.1 to any other skillname that does not contain \"_\", addding a total of hits  4836 ( 0.25175698891144777 %)\n",
      "**Studying market IN\n",
      "Total hits for unique skill names.lower()  21389\n",
      "There are  827 skills within a ld of 0.1 to any other skillname that does not contain \"_\", addding a total of hits  3048 ( 0.14250315582776193 %)\n",
      "There are  3383 skills within a ld of 0.1 to any other skillname that does not contain \"_\", addding a total of hits  6699 ( 0.31319837299546494 %)\n",
      "**Studying market UK\n",
      "Total hits for unique skill names.lower()  21950\n",
      "There are  981 skills within a ld of 0.1 to any other skillname that does not contain \"_\", addding a total of hits  1714 ( 0.0780865603644647 %)\n",
      "There are  3631 skills within a ld of 0.1 to any other skillname that does not contain \"_\", addding a total of hits  5321 ( 0.24241457858769933 %)\n",
      "**Studying market US\n",
      "Total hits for unique skill names.lower()  43410\n",
      "There are  2439 skills within a ld of 0.1 to any other skillname that does not contain \"_\", addding a total of hits  3830 ( 0.08822851877447593 %)\n",
      "There are  8333 skills within a ld of 0.1 to any other skillname that does not contain \"_\", addding a total of hits  11929 ( 0.27479843354065886 %)\n"
     ]
    }
   ],
   "source": [
    "def getFiles(dirName):\n",
    "    listOfFile = os.listdir(dirName)\n",
    "    completeFileList = list()\n",
    "    for file in listOfFile:\n",
    "        completePath = os.path.join(dirName, file)\n",
    "        if os.path.isdir(completePath):\n",
    "            completeFileList = completeFileList + getFiles(completePath)\n",
    "        elif '.json' in completePath:\n",
    "            completeFileList.append(completePath.replace('\\\\', '/'))\n",
    "\n",
    "    return completeFileList\n",
    "\n",
    "#load the jsons\n",
    "alljsons = getFiles('PhonDict/')\n",
    "phondict = {}\n",
    "for f in alljsons:\n",
    "    market = f.split('/')[1].split('.json')[0]\n",
    "    print('Loaded json for ', market, ' :', f)\n",
    "    with open(f, 'r') as json_file:\n",
    "        phondict[market] = json.load(json_file)\n",
    "        \n",
    "#get the data\n",
    "for market in phondict:\n",
    "    print('**Studying market', market)\n",
    "    phondist = phondict[market]['phonetic_distances_minororequal03']\n",
    "    totalcomparisons = phondict[market]['all_distances_processed']\n",
    "    skillnamecount_d = phondict[market]['skillnames_count_d']\n",
    "    totalskills_hits = sum(skillnamecount_d.values())\n",
    "    print('Total hits for unique skill names.lower() ', totalskills_hits)\n",
    "    \n",
    "    '''\n",
    "    find, for every unique name skill.lower(), the minimum distance to any other skill.lower()\n",
    "    '''\n",
    "    min_d_skillname = {}\n",
    "    for skillname in skillnamecount_d.keys():\n",
    "        #print('exploring skill ', skillname)\n",
    "        alldforskill = [(d, sk1, sk2) for (d, sk1, sk2) in phondist if (sk1 == skillname or sk2 == skillname) and d!=0]        \n",
    "        if(len(alldforskill)>0):\n",
    "            min_d_skillname[skillname] = min([d for (d, sk1, sk2) in alldforskill ])\n",
    "        else:\n",
    "            min_d_skillname[skillname] = 1\n",
    "    '''\n",
    "    now, iterate over all unique skills.lower(), and bin them < 0.1 and < 0.2\n",
    "    '''\n",
    "    min_d_skillname_l = [ (k,v) for (k,v) in min_d_skillname.items()]\n",
    "    #get these skills with a minimum lev distance to any other skill name of 0.1\n",
    "    min01 = [(skname, skmin) for (skname, skmin) in min_d_skillname_l if skmin <= 0.1]\n",
    "    #get how many skills are aggregated with that specific name\n",
    "    total01 = sum([skillnamecount_d[skname] for (skname, skmin) in min01])\n",
    "    print('There are ', len(min01), 'skills within a ld of 0.1 to any other skillname that does not contain \"_\", addding a total of hits ', total01, '(',total01/totalskills_hits,'%)')\n",
    "    \n",
    "    #get these skills with a minimum lev distance to any other skill name of 0.1\n",
    "    min02 = [(skname, skmin) for (skname, skmin) in min_d_skillname_l if skmin <= 0.2]\n",
    "    #get how many skills are aggregated with that specific name\n",
    "    total02 = sum([skillnamecount_d[skname] for (skname, skmin) in min02])\n",
    "    print('There are ', len(min02), 'skills within a ld of 0.2 to any other skillname that does not contain \"_\", addding a total of hits ', total02, '(',total02/totalskills_hits,'%)')\n",
    "    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['K', 'AE', 'T', ' ', 'F', 'AE', 'K', 'T', 'S']\n",
      "['F', 'AE', 'T', ' ', 'F', 'AE', 'K', 'T', 'S']\n",
      "0.1111111111111111\n",
      "0.1111111111111111\n",
      "0.2\n"
     ]
    }
   ],
   "source": [
    "w1 = getPhoneticTranslation('Cat Facts')\n",
    "w2 = getPhoneticTranslation('Fat Facts')\n",
    "w3 = getPhoneticTranslation('Kite Facts')\n",
    "w4 = getPhoneticTranslation('fast facts')\n",
    "print(w1)\n",
    "print(w2)\n",
    "print( levdistance(w1,w2) ) \n",
    "print( levdistance(w1,w3) ) \n",
    "print( levdistance(w1,w4) ) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['M', 'IY', ' ', 'HH', 'OW', 'M']\n",
      "['M', 'AY', ' ', 'HH', 'OW', 'M']\n",
      "0.16666666666666666\n"
     ]
    }
   ],
   "source": [
    "w1 = getPhoneticTranslation('mi home')\n",
    "w2 = getPhoneticTranslation('my home')\n",
    "print(w1)\n",
    "print(w2)\n",
    "print( levdistance(w1,w2) ) \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### analyse a few specific examples of phonetically similar skills"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded json for  AU  : PhonDict/AU.json\n",
      "Loaded json for  CA  : PhonDict/CA.json\n",
      "Loaded json for  IN  : PhonDict/IN.json\n",
      "Loaded json for  UK  : PhonDict/UK.json\n",
      "Loaded json for  US  : PhonDict/US.json\n",
      "\n",
      "\n",
      "**Studying market AU\n",
      "[(0.09523809523809523, 'magic 102.5', 'magic 102.9'), (0.1, 'bird trivia', 'nerd trivia'), (0.1, 'fact facts', 'fax facts'), (0.08333333333333333, 'squat workout', 'squats workout'), (0.1, 'bear facts', 'barre facts'), (0.08333333333333333, 'letter i facts', 'letter e facts'), (0.1, 'philly facts', 'film facts'), (0.09090909090909091, 'random skill!', 'random scale'), (0.09090909090909091, 'mango facts', 'tango facts'), (0.09090909090909091, 'number facts', 'number fact'), (0.07692307692307693, 'elephant facts', 'element facts'), (0.1, 'wine trivia', 'jain trivia'), (0.09090909090909091, 'time traveller', 'time travel'), (0.09090909090909091, 'roman facts', 'roma facts'), (0.07692307692307693, 'letter p facts', 'letter c facts'), (0.07692307692307693, 'letter e facts', 'letter c facts'), (0.1, 'tech trivia', 'tick trivia'), (0.08333333333333333, 'fortune cookie', 'fortune cookies'), (0.08333333333333333, 'random facts', 'random fact'), (0.08333333333333333, 'strange  facts', 'strained facts'), (0.1, 'max facts', 'tax facts'), (0.1, 'nepali date', 'nepal date'), (0.1, 'silly facts', 'philly facts'), (0.1, 'train sounds', 'train sound'), (0.1, 'happy fact', 'happy facts'), (0.08333333333333333, 'korea facts', 'korean facts'), (0.07692307692307693, 'arizona facts', 'arizona fact'), (0.1, '96.9 the eagle', '96.1 the eagle'), (0.06666666666666667, 'cosmetic facts', 'cosmetics facts'), (0.08333333333333333, 'random quotes', 'random quote'), (0.1, 'nature sounds', 'nature sound'), (0.08333333333333333, 'decision maker', 'decisionmaker'), (0.09090909090909091, 'cancer facts', 'cancer fact'), (0.09090909090909091, 'state facts', 'states facts'), (0.1, 'kid advice', 'kids advice'), (0.07692307692307693, 'letter v facts', 'letter e facts'), (0.1, 'water facts', 'water fact'), (0.07692307692307693, 'letter n facts', 'letter m facts'), (0.07692307692307693, 'letter v facts', 'letter c facts'), (0.1, 'flight facts', 'light facts'), (0.1, 'facts guru', 'fact guru'), (0.1, 'brain facts', 'rain facts'), (0.1, 'soccer facts', 'soccer fact'), (0.07692307692307693, 'capitals quiz', 'capital quiz'), (0.07692307692307693, 'letter t facts', 'letter e facts'), (0.1, 'awesome facts', 'some facts'), (0.07692307692307693, 'letter g facts', 'letter b facts'), (0.1, 'rowdy radio', 'rudy radio'), (0.09090909090909091, 'roll four dice', 'roll fair dice'), (0.1, 'movies quiz', 'movie quiz'), (0.1, 'asia facts', 'asian facts'), (0.09090909090909091, 'bird trivia', 'birds trivia'), (0.1, 'z93', 'b93'), (0.07692307692307693, 'inventors quiz', 'inventor quiz'), (0.1, 'bunny facts', 'money facts'), (0.1, 'roll the dice', 'roll a dice'), (0.08333333333333333, 'hey exultant', 'hi exultant'), (0.08333333333333333, 'number facts', 'numbers facts'), (0.09090909090909091, 'indian food', 'indian foods'), (0.08333333333333333, 'random fact', 'random facts!'), (0.1, 'zinc facts', 'mink facts'), (0.09090909090909091, 'hey propitious', 'hi propitious'), (0.09090909090909091, 'venus facts', 'venice facts'), (0.09090909090909091, 'facts wizard', 'fact wizard'), (0.08333333333333333, 'plant facts', 'planet facts'), (0.07692307692307693, 'letter n facts', 'letter l facts'), (0.08333333333333333, 'germany facts', 'german facts'), (0.1, 'cat trivia', 'cal trivia'), (0.1, 'delhi facts', 'kelly facts'), (0.09090909090909091, 'desert facts', 'dessert facts'), (0.1, 'shark facts', 'park facts'), (0.1, 'horse facts', 'hawks facts'), (0.1, 'fart sound', 'fart sounds'), (0.08333333333333333, 'kerala facts', 'kerala fact'), (0.07692307692307693, 'letter l facts', 'letter f facts'), (0.07692307692307693, 'letter g facts', 'letter t facts'), (0.1, 'magic words', 'magic word'), (0.1, 'number game', 'numbers game'), (0.07692307692307693, 'letter n facts', 'letter f facts'), (0.1, 'shark facts', 'dark facts'), (0.08333333333333333, 'new year fact', 'new year facts'), (0.1, 'joker facts', 'poker facts'), (0.09090909090909091, 'football teams', 'football terms'), (0.0625, 'beach 104', 'b104'), (0.08333333333333333, 'cobalt facts', 'cobol facts'), (0.07692307692307693, 'elements quiz', 'element quiz'), (0.1, 'mayan facts', 'ryan facts'), (0.09090909090909091, 'russia facts', 'russian facts'), (0.08333333333333333, 'amazing facts', 'amazing fact'), (0.07692307692307693, 'letter v facts', 'letter p facts'), (0.08333333333333333, 'history facts', 'history fact'), (0.07692307692307693, 'bedtime story', 'bedtime stories'), (0.08333333333333333, 'parrot facts', 'parent facts'), (0.1, 'queen facts', 'keen facts'), (0.1, 'india news', 'indian news'), (0.09090909090909091, 'sneaker facts', 'snooker facts'), (0.1, 'buddha quotes', 'buddha quote'), (0.1, 'home workouts', 'home workout'), (0.1, 'darn facts', 'yarn facts'), (0.1, 'number game', 'number games')]\n",
      "\n",
      "[(0.09090909090909091, 'indian food', 'indian foods')]\n",
      "\n",
      "\n",
      "**Studying market CA\n",
      "[(0.07692307692307693, 'letter l facts', 'letter s facts'), (0.09090909090909091, 'bible quotes', 'bible quote'), (0.1, 'bunny facts', 'buddy facts'), (0.07692307692307693, 'letter j facts', 'letter k facts'), (0.1, 'goat facts', 'ghost facts'), (0.07692307692307693, 'letter c facts', 'letter v facts'), (0.1, 'dark facts', 'darn facts'), (0.09090909090909091, 'random skill!', 'random scale'), (0.07692307692307693, 'letter e facts', 'letter g facts'), (0.09090909090909091, 'mango facts', 'tango facts'), (0.09090909090909091, 'battle traps', 'battle tracks'), (0.08333333333333333, 'letter e facts', 'letter i facts'), (0.1, 'mass facts', 'max facts'), (0.1, 'wine trivia', 'jain trivia'), (0.09090909090909091, 'cory facts', 'corgi facts'), (0.1, '99.9 bob fm', '91.9 bob fm'), (0.09090909090909091, 'roman facts', 'roma facts'), (0.1, 'heart facts', 'fart facts!!'), (0.1, 'cal trivia', 'cat trivia'), (0.07692307692307693, 'letter c facts', 'letter p facts'), (0.07692307692307693, 'letter b facts', 'letter t facts'), (0.07692307692307693, 'letter c facts', 'letter g facts'), (0.08333333333333333, 'fortune cookie', 'fortune cookies'), (0.09090909090909091, 'parrot facts', 'ferret facts'), (0.08333333333333333, 'random facts', 'random fact'), (0.08333333333333333, 'strange  facts', 'strained facts'), (0.1, 'train sounds', 'train sound'), (0.07692307692307693, 'letter b facts', 'letter v facts'), (0.07692307692307693, 'arizona facts', 'arizona fact'), (0.09090909090909091, 'number fact', 'number facts'), (0.1, 'home workout', 'home workouts'), (0.08333333333333333, 'bible memory', 'bible memories'), (0.08333333333333333, 'random quotes', 'random quote'), (0.08333333333333333, 'decision maker', 'decisionmaker'), (0.1, 'desi facts', 'delhi facts'), (0.09090909090909091, 'eagle facts', 'seagull facts'), (0.09090909090909091, 'cancer facts', 'cancer fact'), (0.1, 'heart facts', 'fart facts'), (0.09090909090909091, 'state facts', 'states facts'), (0.1, 'water facts', 'water fact'), (0.09090909090909091, 'trump quotes', 'trump quote'), (0.08333333333333333, 'squats workout', 'squat workout'), (0.1, 'kitty facts', 'witty facts'), (0.07692307692307693, 'number trivia', 'numbers trivia'), (0.1, 'brain facts', 'rain facts'), (0.1, 'soccer facts', 'soccer fact'), (0.1, 'poker hands', 'poker hand'), (0.1, 'soccer fact', 'soccer facts!'), (0.1, 'awesome facts', 'some facts'), (0.1, 'rowdy radio', 'rudy radio'), (0.1, 'movies quiz', 'movie quiz'), (0.09090909090909091, 'fruity facts', 'fruits facts'), (0.1, 'marine facts', 'mean facts'), (0.07692307692307693, 'inventors quiz', 'inventor quiz'), (0.1, 'bunny facts', 'money facts'), (0.1, 'roll the dice', 'roll a dice'), (0.08333333333333333, 'hey exultant', 'hi exultant'), (0.08333333333333333, 'cobol facts', 'cobalt facts'), (0.08333333333333333, 'number facts', 'numbers facts'), (0.09090909090909091, 'indian food', 'indian foods'), (0.08333333333333333, 'random fact', 'random facts!'), (0.09090909090909091, 'hey propitious', 'hi propitious'), (0.06666666666666667, 'pakistan facts', 'pakistani facts'), (0.07692307692307693, 'letter e facts', 'letter v facts'), (0.09090909090909091, 'vi reference', 'vine reference'), (0.1, 'dark facts', 'park facts'), (0.09090909090909091, 'hawking facts', 'hacking facts'), (0.08333333333333333, 'germany facts', 'german facts'), (0.1, 'delhi facts', 'kelly facts'), (0.09090909090909091, 'desert facts', 'dessert facts'), (0.1, 'shark facts', 'park facts'), (0.1, 'horse facts', 'hawks facts'), (0.1, 'fart sound', 'fart sounds'), (0.1, 'guppy facts', 'puppy facts'), (0.07692307692307693, 'letter g facts', 'letter t facts'), (0.1, 'mastermind', 'master mind'), (0.09090909090909091, 'little monkey', 'little monk'), (0.08333333333333333, 'planet facts', 'plant facts'), (0.1, 'number game', 'numbers game'), (0.1, 'shark facts', 'dark facts'), (0.08333333333333333, 'new year fact', 'new year facts'), (0.07692307692307693, 'letter f facts', 'letter l facts'), (0.07692307692307693, 'do motivation', 'dude motivation'), (0.09090909090909091, 'football teams', 'football terms'), (0.09090909090909091, 'pick up lines', 'pickup lines'), (0.1, 'tax facts', 'max facts'), (0.1, 'body facts', 'buddy facts'), (0.08333333333333333, 'letter e facts', 'letter o facts'), (0.1, 'toon facts', 'tuna facts'), (0.07692307692307693, 'elements quiz', 'element quiz'), (0.1, 'mayan facts', 'ryan facts'), (0.07692307692307693, 'capital quiz', 'capitals quiz'), (0.09090909090909091, 'dinosaur game', 'dinosaur name'), (0.07692307692307693, 'category game', 'categories game'), (0.08333333333333333, 'amazing facts', 'amazing fact'), (0.08333333333333333, 'history facts', 'history fact'), (0.08333333333333333, 'parrot facts', 'parent facts'), (0.1, 'rainy facts', 'rain facts'), (0.09090909090909091, 'movie ratings', 'movie rating'), (0.1, 'queen facts', 'keen facts')]\n",
      "\n",
      "[(0.09090909090909091, 'indian food', 'indian foods')]\n",
      "\n",
      "\n",
      "**Studying market IN\n",
      "[(0.09090909090909091, 'water facts', 'watery facts'), (0.1, 'weird  facts', 'beard facts'), (0.1, 'foods facts', 'food  facts'), (0.1, 'flute sounds', 'flute sound'), (0.07142857142857142, 'fun fact quiz', 'fun facts quiz'), (0.08333333333333333, 'fact factory', 'facts factory.'), (0.09090909090909091, 'animal name', 'animal names'), (0.08333333333333333, 'iceland facts', 'ireland facts'), (0.1, 'medi facts', 'teddy facts'), (0.07692307692307693, 'suggest movie', 'suggest movies'), (0.07692307692307693, 'company facts', 'company fact'), (0.1, 'goat facts', 'ghost facts'), (0.1, 'lily facts', 'chile facts'), (0.08333333333333333, 'letter i facts', 'letter e facts'), (0.09090909090909091, 'plant facts', 'plant fact'), (0.1, 'on this date', 'on this day'), (0.08333333333333333, 'windows facts', 'window facts'), (0.09090909090909091, 'number facts', 'number fact'), (0.07142857142857142, 'himalaya facts', 'himalayas facts'), (0.07692307692307693, 'ambient sound', 'ambient sounds'), (0.09090909090909091, 'panda facts', 'panda fact'), (0.09090909090909091, 'stars facts', 'stark facts'), (0.09090909090909091, 'top headline', 'top headlines'), (0.09090909090909091, 'sharma quotes', 'dharma quotes'), (0.07692307692307693, 'letter p facts', 'letter c facts'), (0.1, 'food _facts', 'foodie facts'), (0.09090909090909091, 'movies facts', 'movie facts'), (0.07692307692307693, 'letter e facts', 'letter c facts'), (0.08333333333333333, 'fortune cookie', 'fortune cookies'), (0.08333333333333333, 'random facts', 'random fact'), (0.08333333333333333, 'strange  facts', 'strained facts'), (0.09090909090909091, 'mandi facts', 'dandy facts'), (0.09090909090909091, 'unknown facts', 'unknown fact'), (0.1, 'apple facts', 'agile facts'), (0.1, 'train sounds', 'train sound'), (0.08333333333333333, \"friend's fact\", 'f.r.i.e.n.d.s. facts'), (0.09090909090909091, 'knowledge fact', 'knowledge facts'), (0.1, 'foodie facts', 'foods facts'), (0.1, 'dog  facts', 'doggy facts'), (0.09090909090909091, 'random joke', 'random jokes'), (0.1, 'game facts', 'gamer facts'), (0.08333333333333333, 'random quotes', 'random quote'), (0.1, 'brain facts', 'crane facts'), (0.08333333333333333, 'decision maker', 'decisionmaker'), (0.1, 'crane facts', 'rain facts'), (0.09090909090909091, 'g. k. facts', 'v. k. facts'), (0.1, 'silly facts', 'chile facts'), (0.1, 'heart facts', 'fart facts'), (0.08333333333333333, 'plant facts', 'plants facts'), (0.09090909090909091, 'india fact', 'indian fact'), (0.07692307692307693, 'letter v facts', 'letter e facts'), (0.08333333333333333, 'study facts', 'studies facts'), (0.08333333333333333, 'make up facts', 'makeup facts'), (0.07692307692307693, 'letter n facts', 'letter m facts'), (0.09090909090909091, 'hacking facts', 'hawking facts'), (0.09090909090909091, 'sport facts', 'sport fact'), (0.07692307692307693, 'letter v facts', 'letter c facts'), (0.1, 'bear facts', 'berry facts'), (0.08333333333333333, 'squats workout', 'squat workout'), (0.09090909090909091, 'sports fact', 'sport fact'), (0.1, 'brain facts', 'rain facts'), (0.09090909090909091, 'news headline', 'news headlines'), (0.07692307692307693, 'capitals quiz', 'capital quiz'), (0.07692307692307693, 'letter t facts', 'letter e facts'), (0.1, 'awesome facts', 'some facts'), (0.07692307692307693, 'letter g facts', 'letter b facts'), (0.1, 'rowdy radio', 'rudy radio'), (0.09090909090909091, 'roll four dice', 'roll fair dice'), (0.07692307692307693, 'random number', 'random numbers'), (0.08333333333333333, 'magic numbers', 'magic number'), (0.1, 'cats facts', 'cat facts'), (0.1, 'birthday wish', 'birthday wisher'), (0.1, 'bird facts', 'birds facts'), (0.1, 'roll the dice', 'roll a dice'), (0.08333333333333333, 'hey exultant', 'hi exultant'), (0.08333333333333333, 'number facts', 'numbers facts'), (0.07692307692307693, 'indian mystery', 'indian history'), (0.09090909090909091, 'hey propitious', 'hi propitious'), (0.08333333333333333, 'what the fact', 'what the facts'), (0.08333333333333333, 'friends facts', \"friend's fact\"), (0.09090909090909091, 'facts wizard', 'fact wizard'), (0.08333333333333333, 'plant facts', 'planet facts'), (0.1, 'money facts', 'honey facts'), (0.07692307692307693, 'letter n facts', 'letter l facts'), (0.08333333333333333, 'titan facts', 'titans facts'), (0.1, 'delhi facts', 'kelly facts'), (0.07142857142857142, 'the india quiz', 'the indian quiz'), (0.1, 'weird facts', 'beard facts'), (0.09090909090909091, 'logos facts', 'logo facts'), (0.07692307692307693, 'letter l facts', 'letter f facts'), (0.07692307692307693, 'letter g facts', 'letter t facts'), (0.08333333333333333, 'batman facts', 'batman fact'), (0.1, 'goat sounds', 'ghost sounds'), (0.07692307692307693, 'letter n facts', 'letter f facts'), (0.1, 'silk facts', 'milk facts'), (0.07692307692307693, 'do motivation', 'dude motivation'), (0.08333333333333333, 'animal facts', 'animal fact'), (0.1, 'mummy facts', 'yummy facts'), (0.09090909090909091, 'techno facts', 'techno fact'), (0.06666666666666667, 'computer facts', 'computers facts')]\n",
      "\n",
      "[]\n",
      "\n",
      "\n",
      "**Studying market UK\n",
      "[(0.08333333333333333, 'animal sounds', 'animal sound'), (0.08333333333333333, 'penguin joke', 'penguin jokes'), (0.09523809523809523, 'magic 102.5', 'magic 102.9'), (0.07692307692307693, 'magic 8-ball', 'magic hate ball'), (0.1, 'shark facts', 'mark facts'), (0.08333333333333333, 'iceland facts', 'ireland facts'), (0.08333333333333333, 'letter i facts', 'letter e facts'), (0.09090909090909091, 'random skill!', 'random scale'), (0.1, 'fun quotes', 'funny quotes'), (0.09090909090909091, 'number facts', 'number fact'), (0.1, 'ant facts', 'ants facts'), (0.09090909090909091, 'crazy idea', 'crazy ideas'), (0.07692307692307693, 'elephant facts', 'element facts'), (0.09090909090909091, 'time traveller', 'time travel'), (0.09090909090909091, 'times tabler', 'times table'), (0.1, 'pirate 70s', 'spirit 70s'), (0.07692307692307693, 'letter p facts', 'letter c facts'), (0.09090909090909091, 'korea facts', 'cory facts'), (0.07692307692307693, 'letter e facts', 'letter c facts'), (0.08333333333333333, 'fortune cookie', 'fortune cookies'), (0.09090909090909091, 'parrot facts', 'ferret facts'), (0.08333333333333333, 'random facts', 'random fact'), (0.08333333333333333, 'strange  facts', 'strained facts'), (0.1, 'nepali date', 'nepal date'), (0.1, 'mule facts', 'muse facts'), (0.07142857142857142, 'kid activities', 'kids activities'), (0.1, 'silly facts', 'philly facts'), (0.09090909090909091, 'fact india', 'facts india'), (0.1, 'train sounds', 'train sound'), (0.1, 'happy fact', 'happy facts'), (0.06666666666666667, 'i101', 'y101'), (0.1, 'daily facts', 'delhi facts'), (0.1, 'home workout', 'home workouts'), (0.06666666666666667, 'cosmetic facts', 'cosmetics facts'), (0.1, 'pet my rat', 'pet my bat'), (0.1, 'flies facts', 'flight facts'), (0.08333333333333333, 'random quotes', 'random quote'), (0.1, 'brain facts', 'crane facts'), (0.08333333333333333, 'decision maker', 'decisionmaker'), (0.1, 'crane facts', 'rain facts'), (0.09090909090909091, 'eagle facts', 'seagull facts'), (0.1, 'moles facts', 'mole facts'), (0.1, 'kid advice', 'kids advice'), (0.07692307692307693, 'letter v facts', 'letter e facts'), (0.09090909090909091, 'beagle facts', 'beetle facts'), (0.07692307692307693, 'letter n facts', 'letter m facts'), (0.09090909090909091, 'hacking facts', 'hawking facts'), (0.07692307692307693, 'letter v facts', 'letter c facts'), (0.09090909090909091, 'trump quotes', 'trump quote'), (0.1, 'bear facts', 'berry facts'), (0.08333333333333333, 'squats workout', 'squat workout'), (0.1, 'lucky cookies', 'lucky cookie'), (0.07692307692307693, 'number trivia', 'numbers trivia'), (0.1, 'brain facts', 'rain facts'), (0.09090909090909091, 'the love test', 'the luck test'), (0.07692307692307693, 'capitals quiz', 'capital quiz'), (0.1, 'tuna facts', 'toon facts'), (0.07692307692307693, 'letter t facts', 'letter e facts'), (0.1, 'awesome facts', 'some facts'), (0.07692307692307693, 'letter g facts', 'letter b facts'), (0.1, 'movies quiz', 'movie quiz'), (0.09090909090909091, 'fruity facts', 'fruits facts'), (0.07692307692307693, 'random number', 'random numbers'), (0.1, 'asia facts', 'asian facts'), (0.09090909090909091, 'fruity facts', 'fruit facts'), (0.1, 'z93', 'b93'), (0.1, 'mars facts', 'cars facts'), (0.09090909090909091, 'unknown fact', 'unknown facts'), (0.09090909090909091, 'flip the coin', 'flip a coin'), (0.1, 'roll the dice', 'roll a dice'), (0.08333333333333333, 'hey exultant', 'hi exultant'), (0.08333333333333333, 'number facts', 'numbers facts'), (0.1, 'plane facts', 'player facts'), (0.09090909090909091, 'hey propitious', 'hi propitious'), (0.09090909090909091, 'facts wizard', 'fact wizard'), (0.08333333333333333, 'plant facts', 'planet facts'), (0.1, 'kitty facts', 'city facts'), (0.1, 'berry facts', 'gary facts'), (0.07692307692307693, 'letter n facts', 'letter l facts'), (0.1, 'delhi facts', 'kelly facts'), (0.1, 'fart sound', 'fart sounds'), (0.09090909090909091, 'world facts', \"world's facts\"), (0.07692307692307693, 'letter l facts', 'letter f facts'), (0.07692307692307693, 'letter g facts', 'letter t facts'), (0.07692307692307693, 'letter n facts', 'letter f facts'), (0.07692307692307693, 'do motivation', 'dude motivation'), (0.0625, 'z100', 'b100'), (0.0625, 'beach 104', 'b104'), (0.08333333333333333, 'beetle facts', 'beatles facts'), (0.1, 'silly facts', 'city facts'), (0.1, 'iran facts', 'iraq facts'), (0.1, 'world facts', 'word facts'), (0.09090909090909091, 'russia facts', 'russian facts'), (0.08333333333333333, 'amazing facts', 'amazing fact'), (0.07692307692307693, 'letter v facts', 'letter p facts'), (0.1, 'space facts', 'spain facts'), (0.07692307692307693, 'bedtime story', 'bedtime stories'), (0.08333333333333333, 'parrot facts', 'parent facts'), (0.1, 'rainy facts', 'rain facts'), (0.09090909090909091, 'movie ratings', 'movie rating')]\n",
      "\n",
      "[]\n",
      "\n",
      "\n",
      "**Studying market US\n",
      "[(0.1, 'horse sound', 'horse sounds'), (0.09523809523809523, '919 the family', '91.5 the family'), (0.08333333333333333, 'letter i facts', 'letter e facts'), (0.1, 'sweet facts', 'wheat facts'), (0.07692307692307693, 'my flash cards', 'my flashcards'), (0.07692307692307693, 'ambient sound', 'ambient sounds'), (0.09090909090909091, 'panda facts', 'panda fact'), (0.1, 'sun radio', 'sunny radio'), (0.1, 'state quiz', 'states quiz'), (0.1, 'domain check', 'domain checker'), (0.1, 'train sounds', 'train sound'), (0.06666666666666667, 'i101', 'y101'), (0.07142857142857142, 'america facts', 'american facts'), (0.1, 'home workout', 'home workouts'), (0.06666666666666667, 'cosmetic facts', 'cosmetics facts'), (0.1, 'heart facts', 'fart facts'), (0.09090909090909091, 'seattle times', 'seattle time'), (0.08333333333333333, 'plant facts', 'plants facts'), (0.07692307692307693, 'letter v facts', 'letter e facts'), (0.1, 'alt 105.1', 'alt 105.9'), (0.1, 'wool facts', 'wolf facts'), (0.1, 'mix 102.5', 'mix102.9'), (0.1, 'movies quiz', 'movie quiz'), (0.09090909090909091, 'prime number', 'prime numbers'), (0.09090909090909091, 'hongkong news', 'hong kong news'), (0.1, 'birthday wish', 'birthday wisher'), (0.1, 'geese facts', 'greece facts'), (0.1, 'nepali news', 'nepal news'), (0.1, 'mix 102.5', 'kicker 102.5'), (0.09090909090909091, \"mom's story\", 'moms stories'), (0.07142857142857142, 'world capitals', \"world's capitals\"), (0.07692307692307693, 'letter g facts', 'letter t facts'), (0.1, 'la 103.5', 'alt 103.5'), (0.08695652173913043, 'kick 106.7', 'talk 1067'), (0.07692307692307693, 'monthly events', 'monthly event'), (0.0625, 'beach 104', 'b104'), (0.06666666666666667, 'b92.7', 'z92.7'), (0.09090909090909091, 'dinosaur game', 'dinosaur name'), (0.08333333333333333, 'happy new year', \"happy new year's\"), (0.08333333333333333, 'parrot facts', 'parent facts'), (0.1, 'queen facts', 'keen facts'), (0.045454545454545456, 't-100 radio', 'd100 radio'), (0.1, 'number game', 'number games'), (0.09090909090909091, 'now1067', 'i106.7'), (0.08333333333333333, 'sport facts', 'sports facts'), (0.1, 'sleep help', 'sleep helper'), (0.09090909090909091, 'thunder sound', 'thunder sounds'), (0.08333333333333333, 'countries quiz', 'country quiz'), (0.07142857142857142, 'elephant facts', 'elephants facts'), (0.08333333333333333, 'letter o facts', 'letter a facts'), (0.1, 'quake facts', 'quick facts'), (0.1, 'darien patch', 'marion patch'), (0.08333333333333333, 'arts trivia', 'darts trivia'), (0.1, 'smart plug', 'smart plus'), (0.08333333333333333, 'daily proverb', 'daily proverbs'), (0.1, 'great facts', 'grape facts'), (0.09090909090909091, 'sleep sounds', 'sleepy sounds'), (0.07692307692307693, 'compliment me!', 'complement me'), (0.1, 'milton patch', 'wilton patch'), (0.08333333333333333, 'letter a facts', 'letter i facts'), (0.1, 'sports news', 'sport news'), (0.07692307692307693, 'developer news', 'developers news'), (0.1, 'random fun', 'random pun'), (0.09090909090909091, 'spider facts', 'spicer facts'), (0.1, 'movie quote', 'movie quotes'), (0.09090909090909091, 'comics quiz', 'comic quiz'), (0.09090909090909091, 'crypto rates', 'crypto rate'), (0.1, 'rowdy radio', 'ready radio'), (0.07692307692307693, 'letter g facts', 'letter v facts'), (0.09090909090909091, 'adam facts', \"adam's facts\"), (0.08333333333333333, 'head or tails', 'heads or tails?'), (0.1, 'master mind', 'mastermind'), (0.09090909090909091, 'hot 107.5', 'hot 1079'), (0.07142857142857142, 'dramatic sound', 'dramatic sounds'), (0.043478260869565216, '99 7 woof fm', '99.7 woof fm'), (0.1, 'fat facts', 'fast facts'), (0.09523809523809523, 'mix 104.3', 'mix 105.3'), (0.1, 'lorton patch', 'norton patch'), (0.09090909090909091, 'india facts', 'india fact'), (0.1, 'toast master', 'roast master'), (0.08333333333333333, 'crypto track', 'crypto tracker'), (0.1, '98.7 the cove', '98.7 the coast'), (0.09090909090909091, 'river of light', 'river of life'), (0.1, 'money quotes', 'sunny quotes'), (0.09523809523809523, 'joy 107-1', 'i107-1'), (0.1, 'food facts', 'foodie facts'), (0.1, 'space fact', 'space facts!'), (0.07692307692307693, 'important days', 'important day'), (0.07407407407407407, '1071 jack fm', '107.9 jack fm'), (0.1, '96.9 the wolf', '96.9 the bull'), (0.07692307692307693, 'magic 8-ball', 'magic date ball'), (0.08333333333333333, 'letter a facts', 'letter e facts'), (0.08333333333333333, 'period track', 'period tracker'), (0.08333333333333333, 'kansas facts', 'kansas fact'), (0.1, 'snake facts', 'snack facts'), (0.1, 'space facts', 'space fact'), (0.07692307692307693, 'letter k facts', 'letter j facts'), (0.1, 'hi diversion', 'hey diversion'), (0.1, 'fun facts', 'funny facts'), (0.1, 'awesome facts', 'awesome fact')]\n",
      "\n",
      "[(0.1111111111111111, 'food tracker', 'mood tracker'), (0.125, 'good foods', 'good food'), (0.15, 'kids party food ideas', 'kids party game ideas'), (0.09090909090909091, 'indian food', 'indian foods')]\n"
     ]
    }
   ],
   "source": [
    "def getFiles(dirName):\n",
    "    listOfFile = os.listdir(dirName)\n",
    "    completeFileList = list()\n",
    "    for file in listOfFile:\n",
    "        completePath = os.path.join(dirName, file)\n",
    "        if os.path.isdir(completePath):\n",
    "            completeFileList = completeFileList + getFiles(completePath)\n",
    "        elif '.json' in completePath:\n",
    "            completeFileList.append(completePath.replace('\\\\', '/'))\n",
    "\n",
    "    return completeFileList\n",
    "\n",
    "#load the jsons\n",
    "alljsons = getFiles('PhonDict/')\n",
    "phondict = {}\n",
    "for f in alljsons:\n",
    "    market = f.split('/')[1].split('.json')[0]\n",
    "    print('Loaded json for ', market, ' :', f)\n",
    "    with open(f, 'r') as json_file:\n",
    "        phondict[market] = json.load(json_file)\n",
    "        \n",
    "#get the data\n",
    "for market in phondict:\n",
    "    print()\n",
    "    print()\n",
    "    print('**Studying market', market)\n",
    "    phondist = phondict[market]['phonetic_distances_minororequal03']\n",
    "    interestingskills = [(d, sk1, sk2) for (d, sk1, sk2) in phondist if d<=0.1 and d!=0 and len(sk1)<15] \n",
    "    interestingskills2 = [(d, sk1, sk2) for (d, sk1, sk2) in phondist if d<=0.15 and d!=0 and len(sk1)<30 and 'food' in sk1 and 'fact' not in sk1] \n",
    "    print(interestingskills[:100])\n",
    "    print()\n",
    "    print(interestingskills2[:50])\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['B', 'ER', 'D', ' ', 'T', 'R', 'IH', 'V', 'IY', 'AH']\n",
      "['N', 'ER', 'D', ' ', 'T', 'R', 'IH', 'V', 'IY', 'AH']\n",
      "0.1\n"
     ]
    }
   ],
   "source": [
    "w1 = getPhoneticTranslation('bird trivia')\n",
    "w2 = getPhoneticTranslation('nerd trivia')\n",
    "print(w1)\n",
    "print(w2)\n",
    "print( levdistance(w1,w2) ) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['K', 'AE', 'T', ' ', 'F', 'AE', 'K', 'T', 'S']\n",
      "['K', 'AY', 'T', ' ', 'F', 'AE', 'K', 'T', 'S']\n",
      "0.1111111111111111\n"
     ]
    }
   ],
   "source": [
    "w1 = getPhoneticTranslation('cat facts')\n",
    "w2 = getPhoneticTranslation('kite facts')\n",
    "print(w1)\n",
    "print(w2)\n",
    "print( levdistance(w1,w2) ) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
